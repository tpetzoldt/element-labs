[
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html",
    "href": "qmd/05-distributions-fruits-tidyverse.html",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "",
    "text": "The example aims to demonstrate estimation and interpretation of confidence intervals. At the end, the two samples are compared with respect to variance and mean values.\nThe experimental hypotheses was, that weight and size of two samples of Clementine fruits differ. The result is to be visualized with bar charts or box plots. We use only the weight as an example, analysis of the other statistical parameters is left as an optional exercise.\nWe can now derive the following statistical hypotheses about the variance:\n\n\\(H_0\\): The variance of both samples is the same.\n\\(H_a\\): The samples have different variance.\n\nand about the mean:\n\n\\(H_0\\): The mean of both samples is the same.\n\\(H_a\\): The mean values of the samples are different.",
    "crumbs": [
      "Labs",
      "05-Distribution and Confidence Intervals of Clementines"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#prepare-and-inspect-data",
    "href": "qmd/05-distributions-fruits-tidyverse.html#prepare-and-inspect-data",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "3.1 Prepare and inspect data",
    "text": "3.1 Prepare and inspect data\n\nDownload the data set fruits-2023-hse.csv and use one of RStudio’s “Import Dataset” wizards.\nA better alternative is to use read.csv().\n\n\n#  ... do it\n\n\nplot everything, just for testing:\n\n\nplot(fruits)\n\n\nsplit table for box1 and box2:\n\n\nbox1 &lt;- subset(fruits, brand == \"box1\")\nbox2 &lt;- subset(fruits, brand == \"box2\")\n\n\ncompare weight of both groups:\n\n\nboxplot(box1$weight, box2$weight, names=c(\"box1\", \"box2\"))\n\nNote: It is also possible to use boxplot with the model formula syntax. This is the preferred way, because it does not require to split the data set beforehand:\n\nboxplot(weight ~ brand, data = fruits)",
    "crumbs": [
      "Labs",
      "05-Distribution and Confidence Intervals of Clementines"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#check-distribution",
    "href": "qmd/05-distributions-fruits-tidyverse.html#check-distribution",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "3.2 Check distribution",
    "text": "3.2 Check distribution\nWe can check the shape of distribution graphically. If mean values of the samples differ much, it has to be done separately for each sample.\n\n# use `hist`, `qqnorm`, `qqline`\n# ...",
    "crumbs": [
      "Labs",
      "05-Distribution and Confidence Intervals of Clementines"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#sample-statistics",
    "href": "qmd/05-distributions-fruits-tidyverse.html#sample-statistics",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "3.3 Sample statistics",
    "text": "3.3 Sample statistics\nIf we assume normal distribution of the data, we can estimate an approximate prediction interval from the sample parameters, i.e. in which size range we find 95% of the weights within one group.\nWe first calculate mean, sd, N and se for “box1” data set:\n\nbox1.mean &lt;- mean(box1$weight)\nbox1.sd   &lt;- sd(box1$weight)\nbox1.N    &lt;- length(box1$weight)\nbox1.se   &lt;- box1.sd/sqrt(box1.N)\n\nThen we estimate the two-sided 95% prediction interval for the sample, assuming normal distribution:\n\nbox1.95 &lt;- box1.mean + c(-1.96, 1.96) * box1.sd\nbox1.95\n\nInstead of using 1.96, we could also use the quantile function of the normal distribution instead, e.g. qnorm(0.975)for the upper interval or qnorm(c(0.025, 0.975)) for the lower and upper.\nIf the data set is large enough, we can compare the prediction interval from above with the empirical quantiles, i.e. take it directly from the data. Here we do not assume a normal or any other distribution.\n\nquantile(box1$weight, p = c(0.025, 0.975))\n\nNow we plot the data and indicate the 95% interval:\n\nplot(box1$weight)\nabline(h = box1.95, col=\"red\")\n\n… and the same as histogram:\n\nhist(box1$weight)\nabline(v = box1.95, col=\"red\")\nrug(box1$weight, col=\"blue\")",
    "crumbs": [
      "Labs",
      "05-Distribution and Confidence Intervals of Clementines"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#confidence-interval-of-the-mean",
    "href": "qmd/05-distributions-fruits-tidyverse.html#confidence-interval-of-the-mean",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "3.4 Confidence interval of the mean",
    "text": "3.4 Confidence interval of the mean\nThe confidence interval of the mean tells us how precise a mean value was estimated from data. If the sample size is “large enough”, the distribution of the raw data does not necessarily need to be normal distributed, because then mean values tend to approximate a normal distribution due to the central limit theorem.\n\n3.4.1 Confidence interval of the mean for the “box1” data\n\nCalculate the confidence interval of the mean value of the “box1” data set,\nuse +/- 1.96 or (better) the quantile of the t-distribution:\n\n\nbox1.ci &lt;- box1.mean + qt(p = c(0.025, 0.975), df = box1.N-1) * box1.se\n\nNow indicate the confidence interval of the mean in the histogram.\n\nabline(v = box1.ci, col=\"red\")\n\n\n\n3.4.2 Confidence interval for the mean of the “box2” data\nWe could now in principle do the same as above for the “box2” sample, but this would be rather cumbersome and boring. A more efficient method from package dplyr is shown below.",
    "crumbs": [
      "Labs",
      "05-Distribution and Confidence Intervals of Clementines"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#calculation-of-summary-statistics-with-dplyr",
    "href": "qmd/05-distributions-fruits-tidyverse.html#calculation-of-summary-statistics-with-dplyr",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "5.1 Calculation of summary statistics with dplyr",
    "text": "5.1 Calculation of summary statistics with dplyr\nSummarizing can be done with two functions, group_by that adds grouping information to a data frame and summarize to calculate summary statistics. In the following, we use the full data set with 4 groups.\n\nlibrary(\"dplyr\")\nfruits &lt;- read.csv(\"fruits-2023-hse.csv\")\n\nstats &lt;-\n  fruits |&gt;\n    group_by(brand) |&gt;\n    summarize(mean = mean(weight), sd=sd(weight), N=length(weight), se=sd/sqrt(N),\n              lwr = mean + qt(p = 0.025, df = N-1) * se,\n              upr = mean + qt(p = 0.975, df = N-1) * se\n             )\n\nstats",
    "crumbs": [
      "Labs",
      "05-Distribution and Confidence Intervals of Clementines"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#barchart-and-errorbars-with-ggplot2",
    "href": "qmd/05-distributions-fruits-tidyverse.html#barchart-and-errorbars-with-ggplot2",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "5.2 Barchart and errorbars with ggplot2",
    "text": "5.2 Barchart and errorbars with ggplot2\nWe can then use the table of summary statistics directly for a bar chart.\n\nlibrary(\"ggplot2\")\nstats |&gt;\n  ggplot(aes(x=brand, y=mean, min=lwr, max=upr))  +\n    geom_col() + geom_errorbar()",
    "crumbs": [
      "Labs",
      "05-Distribution and Confidence Intervals of Clementines"
    ]
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#additional-tasks",
    "href": "qmd/05-distributions-fruits-tidyverse.html#additional-tasks",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "5.3 Additional tasks",
    "text": "5.3 Additional tasks\nRepeat the analysis with other properties of the fruits, e.g. width and height. Create box plots, analyse distribution, create bar charts.",
    "crumbs": [
      "Labs",
      "05-Distribution and Confidence Intervals of Clementines"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html",
    "href": "qmd/02-discharge-elbe.html",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "",
    "text": "The following practical example demonstrates how data in “long format” can be analysed with R. It builds up on a previous exercise about date and time computation and pivot tables with LibreOfficce.\n\n\nThe example assumes that recent versions of R and RStudio are installed, together with some add-on packages dplyr, tidyr, readr, lubridate and ggplot2. The packages should already be available in the computer pool of the university, otherwise install it over the “Packages” pane in RStudio or from the command line:\nIf all packages are installed, we need to load it to the active session with\n\nlibrary(readr)     # modernized functions to read rectangular data like csv\nlibrary(dplyr)     # the most essential tidyverse packages\nlibrary(tidyr)     # contains for example pivot tables\nlibrary(lubridate) # a tidyverse package for dates\nlibrary(ggplot2)   # high level plotting with the grammar of graphics\n\nThe examples were tested with R versions 4.2.1 – 4.3.2.\n\n\n\nThe data set consists of daily measurements for discharge of the Elbe River in Dresden (daily discharge sum in \\(\\mathrm{m^3 s^{-1}}\\)). The data were kindly provided by the German Federal Institute for Hydrology (BfG)1.\nPlease read the information file elbe_info.txt about data source and copyright before downloading the data file “data.csv”. The data set ist then available in the course folder or from https://github.com/tpetzoldt/datasets/blob/main/data/.\n\n\n\nWe first learn how to import data to R, then we will do date and time conversion and create some plots. After that we learn how to aggregate, analyse and reformat the data set. A final outlook gives an impression how to use pipelines and high level plotting with the ggplot package.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#software-prerequisites",
    "href": "qmd/02-discharge-elbe.html#software-prerequisites",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "",
    "text": "The example assumes that recent versions of R and RStudio are installed, together with some add-on packages dplyr, tidyr, readr, lubridate and ggplot2. The packages should already be available in the computer pool of the university, otherwise install it over the “Packages” pane in RStudio or from the command line:\nIf all packages are installed, we need to load it to the active session with\n\nlibrary(readr)     # modernized functions to read rectangular data like csv\nlibrary(dplyr)     # the most essential tidyverse packages\nlibrary(tidyr)     # contains for example pivot tables\nlibrary(lubridate) # a tidyverse package for dates\nlibrary(ggplot2)   # high level plotting with the grammar of graphics\n\nThe examples were tested with R versions 4.2.1 – 4.3.2.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#the-data-set",
    "href": "qmd/02-discharge-elbe.html#the-data-set",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "",
    "text": "The data set consists of daily measurements for discharge of the Elbe River in Dresden (daily discharge sum in \\(\\mathrm{m^3 s^{-1}}\\)). The data were kindly provided by the German Federal Institute for Hydrology (BfG)1.\nPlease read the information file elbe_info.txt about data source and copyright before downloading the data file “data.csv”. The data set ist then available in the course folder or from https://github.com/tpetzoldt/datasets/blob/main/data/.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#overview",
    "href": "qmd/02-discharge-elbe.html#overview",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "",
    "text": "We first learn how to import data to R, then we will do date and time conversion and create some plots. After that we learn how to aggregate, analyse and reformat the data set. A final outlook gives an impression how to use pipelines and high level plotting with the ggplot package.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#import-of-spreadsheet-and-text-files",
    "href": "qmd/02-discharge-elbe.html#import-of-spreadsheet-and-text-files",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "2.1 Import of spreadsheet and text files",
    "text": "2.1 Import of spreadsheet and text files\nR can access spreadsheet tables and data bases directly using packages like readxl for Excel files. It can also read LibreOffice files and data bases.\nHere we want to make it simple and just read the data from a universal exchange format (.txt or .csv) that can be shared between all systems. In our example, we use a csv-file (comma separated values), where the first row is the table header of unique variable names. The variable names must start with a letter and should not contain special characters, spaces etc. Additional meta information (e.g. source of data) and measurement shold be documented separately, for example in a separate file README.txt.\nThe example file elbe.csv contains daily discharge of the Elbe River in \\(\\mathrm{m^3 s^{-1}}\\) from gauging station Dresden, river km 55.6 from the Federal Waterways and Shipping Administration (WSV) and where provided by the Federal Institute for Hydrology (BfG).\nThe third column “validated” indicates whether the values were finally approved by WSV and BfG. Data from the 19th century are particularly uncertain. Please consult the file elbe_info.txt for details.\n\n\n\nRStudio import assistant",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#input-method-1-use-the-import-dataset-wizard-of-rstudio",
    "href": "qmd/02-discharge-elbe.html#input-method-1-use-the-import-dataset-wizard-of-rstudio",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "2.2 Input method 1: Use the import dataset wizard of RStudio",
    "text": "2.2 Input method 1: Use the import dataset wizard of RStudio\n\nFirst, download the file elbe.csv and store it to your working directory.\nNow Open RStudio and Select: File – Import Dataset – From Text (readr).\nOpen the file and you will see the import dataset assistant. Select the correct settings for your file and choose an appropriate name (e.g. elbe) for the data frame in R.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#input-method-2-read-data-directly-from-r",
    "href": "qmd/02-discharge-elbe.html#input-method-2-read-data-directly-from-r",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "2.3 Input method 2: Read data directly from R",
    "text": "2.3 Input method 2: Read data directly from R\n\nNavigate to the data file with the “files pane” (bottom right in Rstudio by default),\nIf you cannot find the file easily, use the dots (…) of the file pane.\nSelect: More – Set as Working Directory.\nRun the following commands in R:\n\n\nlibrary(\"readr\")\nelbe &lt;- read_csv(\"elbe.csv\")\n\nThis works if the data format is a true csv (comma separated values) file with English decimal dot “.” for the numbers and “,” for the column separator. If the file format is different, we may use read.table, a more flexible function that allows to specify the column separator decimal.\nNote: for the exercise, one of the above methods is sufficient, either the import wizard or read_csv. The command line method is advantageous if a file is read several times or if several files need to be imported.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#date-and-time-conversion",
    "href": "qmd/02-discharge-elbe.html#date-and-time-conversion",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.1 Date and time conversion",
    "text": "3.1 Date and time conversion\nIn the following we extend the elbe data frame by adding information about the day, month, year and day of year. Here function mutate adds additional columns, or modifies existing if the column names exist.\nNote also that the day of year function in the date and time package lubridate is named yday. Details about date and time conversion can be found in a cheatsheet available from https://raw.githubusercontent.com/rstudio/cheatsheets/main/lubridate.pdf\n\nelbe &lt;- mutate(elbe,\n               date  = as.Date(date), # may be redundant if read_csv was used\n               day   = day(date), \n               month = month(date), \n               year  = year(date), \n               doy   = yday(date))\n\nNow, have a look at the “Global Environment” pane and inspect the data structure of the elbe data frame.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#basic-plotting-with-rs-base-plot",
    "href": "qmd/02-discharge-elbe.html#basic-plotting-with-rs-base-plot",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.2 Basic plotting with R’s base plot",
    "text": "3.2 Basic plotting with R’s base plot\nThe full time series can be plotted using the date as argument for the x-axis and discharge for the y-axis. The $ sign indicates from which column of the elbe-table data are taken. The \"l\" indicates line plots.\n\nplot(elbe$date, elbe$discharge, type=\"l\")\n\nThe same can be done with a so-called formula syntax. Here y and x are given in opposite order, separated with a ~ (tilde sign). It can be read as “y as a function of x”. The formula syntax allows to specify the data as a separate argument.\n\nplot(discharge ~ date, data=elbe, type=\"l\")\n\n\n\n\n\n\n\n\nThe formula syntax has additional benefits, for example a subset argument:\n\nplot(discharge ~ doy, data=elbe, subset = year==2002, col=\"blue\", type=\"l\")\nlines(discharge ~ doy, data=elbe, subset = year==2003, col=\"red\")\n\nExercise: Plot 4 years with 4 different colors, 2 wet and 2 dry years.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#histograms",
    "href": "qmd/02-discharge-elbe.html#histograms",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.3 Histograms",
    "text": "3.3 Histograms\nHistograms show the distribution of the data. Compare the shape of following three:\n\nHistogram with untransformed data\nHistogram with log-transformed data\nHistogram with log-transformed data, where a certain baseflow is subtracted before taking the log.\n\n\nhist(elbe$discharge)\nhist(log(elbe$discharge))\nhist(log(elbe$discharge - 0.9 * min(elbe$discharge)))\n\nExercises:\n\nDiscuss, which of the three histograms best describe discharge distribution.\nRepeat the plot with smaller classes, e.g. hist(elbe$discharge, breaks=50).",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#boxplots",
    "href": "qmd/02-discharge-elbe.html#boxplots",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.4 Boxplots",
    "text": "3.4 Boxplots\nBoxplots are a very compact way to visualize the distribution of data:\n\nboxplot(elbe$discharge)\n\nExercise: Create boxplots for:\n\nlog-transformed discharge,\nlog-transformed value of discharge - baseflow.\nInterpret the results: What do the “middle line”, the box, the whiskers and the extreme values tell us?\nDiscuss the “outliers”: how many, at which side and if they are really “outliers”.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#cumulative-sums",
    "href": "qmd/02-discharge-elbe.html#cumulative-sums",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "3.5 Cumulative sums",
    "text": "3.5 Cumulative sums\nAnnual cumulative sum plots are a hydrological standard tool used by reservoir managers. We can use the R function cumsum, that by successive cumulation converts a sequence of:\n\\(x_1, x_2, x_3, x_4, \\dots\\) into\n\\((x_1), (x_1+x_2), (x_1+x_2+x_3), (x_1+x_2+x_3+x_4), \\dots\\)\nIf we just use cumsum for daily discharge (in \\(\\mathrm{m^3 s^{-1}}\\)) and multiply it with the number of seconds per day / 1e6, we get a cumulative sum in Mio \\(\\mathrm{m^3}\\) over all years:\n\nelbe$cum &lt;- cumsum(elbe$discharge) * 60*60*24 / 1e6\nplot(elbe$date, elbe$cum, type=\"l\", ylab=\"Mio m^3\")\n\nHowever, cumulation is more commonly done per year, i.e. each year should start with the discharge from a given start day. In the following, let’s start with 1st of January, experts may consider to modify the code, to use the German hydrological year.\n\none_year     &lt;- subset(elbe, year == 2000)\none_year$cum &lt;- cumsum(one_year$discharge) * 60*60*24 / 1e6\nplot(one_year$date, one_year$cum, type=\"l\", ylab=\"Mio m^3\")\n\nHere, a steep increase shows a wet period, a flat curve indicates a dry period.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#summaries-and-cross-tabulation",
    "href": "qmd/02-discharge-elbe.html#summaries-and-cross-tabulation",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "4.1 Summaries and cross-tabulation",
    "text": "4.1 Summaries and cross-tabulation\nHere we use the tidyverse method summarize, after grouping with group_by. It is, compared to the classical aggregate-function i R more powerful and much easier to use:\n\n## calculate annual mean, minimum, maximum\nelbe_grouped &lt;- group_by(elbe, year)\n\ntotals &lt;- summarize(elbe_grouped, \n            mean = mean(discharge), \n            min = min(discharge), \n            max = max(discharge))\ntotals\n\nExercise: Use the above method to compute annual total discharge sums and monthly average discharge values.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#a-standard-pivot-table",
    "href": "qmd/02-discharge-elbe.html#a-standard-pivot-table",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "4.2 A standard pivot table",
    "text": "4.2 A standard pivot table\nTidyverse provides also tools for the conversion of data base tables (long data format) into cross-tables (wide data format) and vice versa. This functionality changed several times in the last years, so you may see functions like melt and cast or gather and spread doing more or less the same, but with different syntax. The most recent development suggests the two functions pivot_wider and pivot_longer for this purpose.\nIts first argument is a data base table, the other arguments define the structure of the desired crosstable.\nHere id_cols is the name of a column in a long table that will become the rows, names_from indicates where the names of the columns are taken from and values_from the column with the values for the cross table. If more than one value is possible for a row x column combination, an optional values_fn can be given.\n\nelbe_wide &lt;-  pivot_wider(elbe, \n                id_cols = doy, \n                names_from = year, \n                values_from = discharge, \n                #values_fn = mean\n              )\nelbe_wide\n\nExercise: Create a crosstable for monthly max. discharge over all years.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#back-conversion-of-a-crosstable-into-a-data-base-table",
    "href": "qmd/02-discharge-elbe.html#back-conversion-of-a-crosstable-into-a-data-base-table",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "4.3 Back-conversion of a crosstable into a data base table",
    "text": "4.3 Back-conversion of a crosstable into a data base table\nThe inverse case is also possible, e.g. the conversion of a cross table into a data base table. It can be done with the function pivot_longer. The column of the id.vars variable(s) will become identifier(s) downwards.\n\npivot_longer(elbe_wide, names_to=\"year\", cols=as.character(1989:2019))",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#minimum-maximum-plot-with-summarize-and-ggplot2",
    "href": "qmd/02-discharge-elbe.html#minimum-maximum-plot-with-summarize-and-ggplot2",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "5.1 Minimum-Maximum plot with summarize and ggplot2",
    "text": "5.1 Minimum-Maximum plot with summarize and ggplot2\n\n## Read data\nelbe &lt;- read.csv(\"elbe.csv\")\n\n## do everything in one pipeline:\n##   doy calculation; grouping; min, max, mean; melt to long format; plotting\nelbe |&gt; \n  mutate(doy = yday(date)) |&gt;\n  group_by(doy) |&gt;\n  summarize(max = max(discharge), \n            mean = mean(discharge), \n            min = min(discharge)) |&gt;\n  pivot_longer(cols = c(\"min\", \"mean\", \"max\"), \n               names_to = \"statistic\", \n               values_to = \"discharge\") |&gt;\n  ggplot(aes(doy, discharge, color = statistic)) + geom_line()",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#cumulative-sums-for-all-years",
    "href": "qmd/02-discharge-elbe.html#cumulative-sums-for-all-years",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "5.2 Cumulative sums for all years",
    "text": "5.2 Cumulative sums for all years\nCumulative sums are a standard tool used by hydrologists and reservoir managers. They allow to detect easily dry and wet years and periods.\nIf we just use cumsum, we get a cumulative sum over all years:\n\nelbe |&gt; \n  mutate(doy = yday(date), year = year(date)) |&gt;\n  filter(year %in% 2000:2010) |&gt;\n  group_by(year = factor(year)) |&gt;\n  mutate(cum_discharge = cumsum(discharge) * 60*60*24) |&gt;\n  ggplot(aes(doy, cum_discharge, color = year)) + geom_line()\n\n\n\n\n\n\n\n\nExercises:\n\nWhich year was the wettest, which one the driest year in total? Find a year with dry spring and wet summer. Use the outcommented filter to reduce the number of simultanaeous lines.\nModify the commands so that the hydrological year is shown. Note that the German hydrological year goes from 1st November to 31st October of the following year. Other countries have different regulations.",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe.html#footnotes",
    "href": "qmd/02-discharge-elbe.html#footnotes",
    "title": "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData Source: Federal Waterways and Shipping Administration (WSV), provided by the Federal Institute for Hydrology (BfG).↩︎",
    "crumbs": [
      "Labs",
      "02-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html",
    "href": "qmd/12-timeseries-trends.html",
    "title": "12-An introductory time series example",
    "section": "",
    "text": "The main scientific question of the following examples is the existence of a trend. However, most trend tests assume stationarity of the residuals, so the concept of stationarity is first introduced by means of two artificial data sets. Here we introduce the following concepts:\n\ntrend stationarity and difference stationarity\nautocorrelation and partial autocorrelation\ntest for stationarity\ntest for a monotonic trend\n\nThe general procedure should then be applied to two real data sets as an exercise. Please keep in mind that the main objective here is trend analysis. The concepts of stationarity and autocorrelation and the related tests are only used as pre-tests to check if simple trend tests are possible. Please note also the importance of the effect size, e.g. the temperature increase pear year.\nThe book of Kleiber & Zeileis (2008) contains an excellent explanation of the methods described here and is strongly recommended for further reading.",
    "crumbs": [
      "Labs",
      "12-An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#data-set",
    "href": "qmd/12-timeseries-trends.html#data-set",
    "title": "12-An introductory time series example",
    "section": "2.1 Data set",
    "text": "2.1 Data set\nThe data set “timeseries.txt” contains artificial data with specifically designed properties, similar to the TSP and DSP series in the tutorial https://tpetzoldt.github.io/elements/. The data can be entered to R either with read.table or with the import assistant, then we convert it to time series objects (ts), to make their analysis easier.\n\ndat &lt;- read.csv(\"timeseries.csv\", header = TRUE)\nTSP &lt;- ts(dat$TSP)\nDSP &lt;- ts(dat$DSP)\n\nIt is always a good idea to plot the data first.\n\npar(mfrow=c(1,2))\nplot(TSP)\nplot(DSP)",
    "crumbs": [
      "Labs",
      "12-An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#autocorrelation-and-partial-autocorrelation",
    "href": "qmd/12-timeseries-trends.html#autocorrelation-and-partial-autocorrelation",
    "title": "12-An introductory time series example",
    "section": "2.2 Autocorrelation and partial autocorrelation",
    "text": "2.2 Autocorrelation and partial autocorrelation\nFirst, plot the autocorrelation (acf) and partial autocorrelation (pacf) of the DSP series:\n\npar(mfrow=c(1,2))\nacf(DSP)\npacf(DSP)\n\n\n\n\n\n\n\n\n… and interpret the results.\nThen plot the acf for both series, together with the autocorrelation of the differenced and residual time series:\n\npar(mfrow=c(2,3))\nacf(TSP)\nacf(diff(TSP))\nacf(residuals(lm(TSP~time(TSP))))\nacf(DSP)\nacf(diff(DSP))\nacf(residuals(lm(DSP~time(DSP))))\n\nHere, diff is used for differencing the time series i.e. to compute differences between consecutive values, while lm fits a linear regression from which residuals extracts the residuals.\nThe autocorrelation function acf can be used to identify specific patterns. A series is considered as approximately stationary, if all autocorrelations (except for \\(lag=0\\)) are “almost non-significant”.\nHint: Deconstruct the parenthetisized statements like acf(residuals(lm(TSP~time(TSP)))) into 3 separate lines to understand better what they do. Plot the data and the trend.",
    "crumbs": [
      "Labs",
      "12-An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#stationarity-test",
    "href": "qmd/12-timeseries-trends.html#stationarity-test",
    "title": "12-An introductory time series example",
    "section": "2.3 Stationarity test",
    "text": "2.3 Stationarity test\nThe Kwiatkowski-Phillips-Schmidt-Shin test checks directly for stationarity, where \\(H_0\\) may be either level stationarity or trend stationarity. Don’t get confused:\n\nlevel stationary is just the same as stationary, the additional “level” just makes it clearer.\nin contrast, trend stationary is essentially non-stationary, but can easily be made stationary by subtracting a trend, because the residuals are stationary.\nthe warning message of the KPSS test is normal and not an “error”, its just an information that the true p-value is either smaller or greater than the printed value.\n\n\nlibrary(\"tseries\")\nkpss.test(TSP, null=\"Level\") # instationary\nkpss.test(TSP, null=\"Trend\") # stationary after trend removal\nkpss.test(DSP, null=\"Level\") # instationary\nkpss.test(DSP, null=\"Trend\") # still instationary\nkpss.test(diff(DSP), null=\"Level\")",
    "crumbs": [
      "Labs",
      "12-An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#mann-kendall-test-for-trends",
    "href": "qmd/12-timeseries-trends.html#mann-kendall-test-for-trends",
    "title": "12-An introductory time series example",
    "section": "2.4 Mann-Kendall test for trends",
    "text": "2.4 Mann-Kendall test for trends\nThis is now finally the main test.\n\nlibrary(\"Kendall\")\nMannKendall(TSP) # correct only for trend stationary time series\nMannKendall(DSP) # wrong, because time series was difference stationary",
    "crumbs": [
      "Labs",
      "12-An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#trend-of-air-temperature",
    "href": "qmd/12-timeseries-trends.html#trend-of-air-temperature",
    "title": "12-An introductory time series example",
    "section": "3.1 Trend of air temperature",
    "text": "3.1 Trend of air temperature\nThe plot seems to show an increasing trend, especially since 1980.\n\nplot(Tair)\n\nTest of stationarity\nTest for stationarity for the original and the trend adjusted series graphically with acf and quantitatively with the KPSS test:\n\nkpss.test(Tair)\n\nWe see that \\(p &lt; 0.01\\) so it is not “level stationary”!\nBut if we allow for a trend:\n\nkpss.test(Tair, null=\"Trend\")\n\n… we get \\(p &gt; 0.05\\) i.e. it is trend stationary (stationary after trend removal). Therefore, a trend test is possible.\nTrend test\nWe use the Mann-Kendall test dirst, that tests for monotonous trends:\n\nMannKendall(Tair)\n\nNow we fit a linear model to find out how much the temperature increased per day during this time.\n\nm &lt;- lm(Tair ~ time(Tair))\nsummary(m)\nplot(Tair)\nabline(m, col=\"red\")\n\nNow test the assumptions. Firstly test that the residuals have no autocorrelation:\n\nacf(residuals(m))\n\nOptional Task: use additional diagnostics, e.g. plot residuals versus fitted or qqnorm(residuals(m)) to test for normal distribution. Write the results down and evaluate what they can tell us.\nQuestions:\n\nIs the trend significant?\nwhich of the used tests is the best to test for a trend?\nwhat does “monotonous” mean?\nWhat is the advantage of fitting a linear model?\nWhat is the purpose of checking autocorrelation of the residuals with acf?",
    "crumbs": [
      "Labs",
      "12-An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#stationarity-and-trend-of-water-temperature",
    "href": "qmd/12-timeseries-trends.html#stationarity-and-trend-of-water-temperature",
    "title": "12-An introductory time series example",
    "section": "3.2 Stationarity and trend of water temperature",
    "text": "3.2 Stationarity and trend of water temperature\nNow repeat the same for the water temperature data, interpret the results and write a short report. Read about limnology of stratified lakes in temperate climate zones and discuss reasons why the trend of water temperature is weaker or stronger than air temperature.\nScientific Questions\n\nWas there a significant trend in water and air temperature?\nHow much Kelvin (degrees centigrade) did the water temperature increase on average during this time?\nWas the trend of water weaker or stronger than for air temperature? Which lake-physical processes are responsible for this effect?",
    "crumbs": [
      "Labs",
      "12-An introductory time series example"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression.html",
    "href": "qmd/10-nonlinear-regression.html",
    "title": "10-Nonlinear Regression",
    "section": "",
    "text": "The following examples demonstrate how to perform non-linear regression in R. This is quite different from linear regression, not only because the regression functions show a curve, but also due to the applied numerical techniques. While in the linear case, the coefficients of the regression line can be calculated directly (analytically) by solving a linear system of equations, iterative numerical optimization needs to be used instead.\nThis means that the coefficients are approximated step by step until convergence, beginning with start values specified by the user. There is no guarantee that an optimal solution can be found.\nThe following examples are intended as a starting point, the last example (logistic growth) is left as an exercise.\n\n\nThe first example shows an exponentially growing data set that is fitted by nonlinear least squares (nls). Here, the exponential function is given directly at the right hand side of the formula and the start values for the parameters are specified in pstart. An optional argument trace = TRUE is set to watch the progress of iteration.\nFunction predict can be used to create dense \\(x\\) and \\(y\\)-values (here: x1 and y1) for a smooth curve.\nThe statistical results of the regression are displayed with summary: parameters (\\(a\\), \\(b\\)) of the curve, standard errors and the correlation between \\(a\\) and \\(b\\). Note the optional correlation=TRUE argument. High correlations can indicate problems in model fitting, especially due to so called non-identifiablility of the parameter set.\nHere everything went through even with high correlation, because the data do not vary much around the exponential curve.\nFinally, the nonlinear coefficient of determination \\(R^2\\) can be calculated from the variances of the residuals and the original data:\n\nx &lt;- 1:10\ny &lt;- c(1.6, 1.8, 2.1, 2.8, 3.5, 4.1, 5.1, 5.8, 7.1, 9.0)\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ a * exp(b * x), start = pstart, trace = TRUE)\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\n\nsummary(model_fit, correlation = TRUE)\n\n## R squared\n1 - var(residuals(model_fit))/var(y)\n\n\n\n\nInstead of putting the regression model directly into nls it is also possible to use a user-defined function, that we call f here for example. This approach is especially useful for more complicated regression models:\n\nf &lt;- function(x, a, b) {a * exp(b * x)}\n\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ f(x, a, b), start = pstart)\n\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\nsummary(model_fit, correlation = TRUE)\n1 - var(residuals(model_fit))/var(y)",
    "crumbs": [
      "Labs",
      "10-Nonlinear Regression"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression.html#exponential-growth",
    "href": "qmd/10-nonlinear-regression.html#exponential-growth",
    "title": "10-Nonlinear Regression",
    "section": "",
    "text": "The first example shows an exponentially growing data set that is fitted by nonlinear least squares (nls). Here, the exponential function is given directly at the right hand side of the formula and the start values for the parameters are specified in pstart. An optional argument trace = TRUE is set to watch the progress of iteration.\nFunction predict can be used to create dense \\(x\\) and \\(y\\)-values (here: x1 and y1) for a smooth curve.\nThe statistical results of the regression are displayed with summary: parameters (\\(a\\), \\(b\\)) of the curve, standard errors and the correlation between \\(a\\) and \\(b\\). Note the optional correlation=TRUE argument. High correlations can indicate problems in model fitting, especially due to so called non-identifiablility of the parameter set.\nHere everything went through even with high correlation, because the data do not vary much around the exponential curve.\nFinally, the nonlinear coefficient of determination \\(R^2\\) can be calculated from the variances of the residuals and the original data:\n\nx &lt;- 1:10\ny &lt;- c(1.6, 1.8, 2.1, 2.8, 3.5, 4.1, 5.1, 5.8, 7.1, 9.0)\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ a * exp(b * x), start = pstart, trace = TRUE)\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\n\nsummary(model_fit, correlation = TRUE)\n\n## R squared\n1 - var(residuals(model_fit))/var(y)",
    "crumbs": [
      "Labs",
      "10-Nonlinear Regression"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression.html#regression-model-as-user-defined-function",
    "href": "qmd/10-nonlinear-regression.html#regression-model-as-user-defined-function",
    "title": "10-Nonlinear Regression",
    "section": "",
    "text": "Instead of putting the regression model directly into nls it is also possible to use a user-defined function, that we call f here for example. This approach is especially useful for more complicated regression models:\n\nf &lt;- function(x, a, b) {a * exp(b * x)}\n\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ f(x, a, b), start = pstart)\n\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\nsummary(model_fit, correlation = TRUE)\n1 - var(residuals(model_fit))/var(y)",
    "crumbs": [
      "Labs",
      "10-Nonlinear Regression"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html",
    "href": "qmd/06-classical-tests.html",
    "title": "06-Classical Tests",
    "section": "",
    "text": "The example is inspired by a classical test data set [@Student1908] about a study with two groups of persons treated with two different pharmaceutical drugs.\nDrug 1: 8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10\nDrug 2: 9.9, 8.8, 9.1, 8.1, 7.9, 12.4, 13.5, 9.6, 12.6, 11.4\nThe data are the duration of sleeping time in hours. It is assumed that the normal sleeping time would be 8 hours.\n\n\nLet’s test whether the drugs increased or decreased sleeping time, compared to 8 hours:\n\n x &lt;- c(8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10)\n t.test(x, mu = 8)\n\nExercise 1: Test the effect of the second drug. Does it change sleeping duration?\n\n\n\nThe two sample t-Test is used to compare two groups of data: Related to our example, we test the following hypotheses:\n\\(H_0\\): Both drugs have the same effect.\n\\(H_A\\): The drugs have a different effect, i.e. one of the drugs is stronger.\n\nx1 &lt;- c(8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10)\nx2 &lt;- c(9.9, 8.8, 9.1, 8.1, 7.9, 12.4, 13.5, 9.6, 12.6, 11.4)\nt.test(x1, x2)  # Welch-t-test\n\nHere, R performs the Welch test by default, that is also valid for samples with different variances.\nThe classical approach suggested to check homogeneity of variances with the F-Test (var.test) first and if the assumption holds, to apply the “ordinary” two sample t-test (t.test(....., var.equal = TRUE)). This method is not anymore recommended [@Delacre2017].\n\nvar.test(x1, x2)                    # F-test as pre-test\nt.test(x1, x2, var.equal = TRUE)    # classical t-test\n\nExercise 2: Create a boxplot, and perform the tests. What is the effect size, i.e. by how many hours differs sleep duration?\n\n\n\nGiven is a number of students that passed an examination in statistics. The examination was written two times, one time before and one time after an additional series of lectures. The values represent the numbers of points approached during the examination. Check whether the additional lectures had any positive effect:\n\nx1 &lt;- c(69,  77, 35, 34, 87, 45, 95, 83)\nx2 &lt;- c(100, 97, 67, 42, 75, 73, 92, 97)\n\nExercise 3: The test was conducted by the same individuals before and after the course, so one can use a paired t-test:\n\nt.test(x1, x2, paired = TRUE)\n\nThen compare the results with the ordinary two-sample t-test.\n\n\n\nThe Mann-Whitney and Wilxon tests are nonparametric tests of location. “Nonparametric” means, that the general location of the distributions is compared and not a parameter like the mean. This makes the test independent of assumptions, but can sometimes lead to a vague interpretation.\nExercise 4: Now repeat the comparison for the sleep study using the Wilcoxon test for unpaired and paired samples. Note that the unpaired test is often also called “Mann-Whitney U test”.\nIn R both tests can be found as wilcox.test. Use the help system of R (?wilcox.test) and read the help page about the usage of these tests.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#one-sample-t-test",
    "href": "qmd/06-classical-tests.html#one-sample-t-test",
    "title": "06-Classical Tests",
    "section": "",
    "text": "Let’s test whether the drugs increased or decreased sleeping time, compared to 8 hours:\n\n x &lt;- c(8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10)\n t.test(x, mu = 8)\n\nExercise 1: Test the effect of the second drug. Does it change sleeping duration?",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#two-sample-t-test",
    "href": "qmd/06-classical-tests.html#two-sample-t-test",
    "title": "06-Classical Tests",
    "section": "",
    "text": "The two sample t-Test is used to compare two groups of data: Related to our example, we test the following hypotheses:\n\\(H_0\\): Both drugs have the same effect.\n\\(H_A\\): The drugs have a different effect, i.e. one of the drugs is stronger.\n\nx1 &lt;- c(8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10)\nx2 &lt;- c(9.9, 8.8, 9.1, 8.1, 7.9, 12.4, 13.5, 9.6, 12.6, 11.4)\nt.test(x1, x2)  # Welch-t-test\n\nHere, R performs the Welch test by default, that is also valid for samples with different variances.\nThe classical approach suggested to check homogeneity of variances with the F-Test (var.test) first and if the assumption holds, to apply the “ordinary” two sample t-test (t.test(....., var.equal = TRUE)). This method is not anymore recommended [@Delacre2017].\n\nvar.test(x1, x2)                    # F-test as pre-test\nt.test(x1, x2, var.equal = TRUE)    # classical t-test\n\nExercise 2: Create a boxplot, and perform the tests. What is the effect size, i.e. by how many hours differs sleep duration?",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#paired-t-test",
    "href": "qmd/06-classical-tests.html#paired-t-test",
    "title": "06-Classical Tests",
    "section": "",
    "text": "Given is a number of students that passed an examination in statistics. The examination was written two times, one time before and one time after an additional series of lectures. The values represent the numbers of points approached during the examination. Check whether the additional lectures had any positive effect:\n\nx1 &lt;- c(69,  77, 35, 34, 87, 45, 95, 83)\nx2 &lt;- c(100, 97, 67, 42, 75, 73, 92, 97)\n\nExercise 3: The test was conducted by the same individuals before and after the course, so one can use a paired t-test:\n\nt.test(x1, x2, paired = TRUE)\n\nThen compare the results with the ordinary two-sample t-test.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#wilcoxon-test-optional",
    "href": "qmd/06-classical-tests.html#wilcoxon-test-optional",
    "title": "06-Classical Tests",
    "section": "",
    "text": "The Mann-Whitney and Wilxon tests are nonparametric tests of location. “Nonparametric” means, that the general location of the distributions is compared and not a parameter like the mean. This makes the test independent of assumptions, but can sometimes lead to a vague interpretation.\nExercise 4: Now repeat the comparison for the sleep study using the Wilcoxon test for unpaired and paired samples. Note that the unpaired test is often also called “Mann-Whitney U test”.\nIn R both tests can be found as wilcox.test. Use the help system of R (?wilcox.test) and read the help page about the usage of these tests.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#perform-a-statistical-test-for-the-following-hypotheses",
    "href": "qmd/06-classical-tests.html#perform-a-statistical-test-for-the-following-hypotheses",
    "title": "06-Classical Tests",
    "section": "2.1 Perform a statistical test for the following hypotheses",
    "text": "2.1 Perform a statistical test for the following hypotheses\n\\(H_0\\): The weight of the fruits bought on Friday (box2) and on Monday (box1) are the same.\n\\(H_A\\): The weight of the fruits is different.\nSelect a proper statistical test and interpret its results.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#calculate-absolute-and-relative-effect-sizes",
    "href": "qmd/06-classical-tests.html#calculate-absolute-and-relative-effect-sizes",
    "title": "06-Classical Tests",
    "section": "2.2 Calculate absolute and relative effect sizes",
    "text": "2.2 Calculate absolute and relative effect sizes\n\nCalculate the mean values of both samples \\(\\bar{x}_{1}, \\bar{x}_{2}\\) and the absolute effect size:\n\n\\[\\Delta=\\bar{x}_{1}-\\bar{x}_{2}\\].\n\nCalculate the pooled standard deviation (\\(N_1, N_2\\) = sample size, \\(s_1, s_2\\)= standard deviation):\n\n\\[s_{1,2} = \\sqrt{\\frac{(N_1 - 1) s_1^2 + (N_2 - 1) s_2^2)}{N_1 + N_2 - 2}}\\]\n\nCalculate the relative effect size as\n\n\\[\\delta=\\frac{|\\bar{x}_{1}-\\bar{x}_{2}|}{s_{1,2}}\\].\n\nRead in Wikipedia about Cohen’s d and other measures of effect size.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#introduction",
    "href": "qmd/06-classical-tests.html#introduction",
    "title": "06-Classical Tests",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nTaken from Agresti (2002), Fisher’s Tea Drinker:\n“A British woman claimed to be able to distinguish whether milk or tea was added to the cup first. To test, she was given 8 cups of tea, in four of which milk was added first. The null hypothesis is that there is no association between the true order of pouring and the woman’s guess, the alternative that there is a positive association (that the odds ratio is greater than 1).”\nThe experiment revealed the following outcome: With tea first, the tea taster identified three times the correct answer and was one time wrong and the same occurred with milk first (3 true, 1 wrong).\nFor a TV show this would be sufficient, but how big was the probability to get such a result just by chance?",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#methods-and-results",
    "href": "qmd/06-classical-tests.html#methods-and-results",
    "title": "06-Classical Tests",
    "section": "3.2 Methods and Results",
    "text": "3.2 Methods and Results\nWe put the data into a matrix:\n\nx &lt;- matrix(c(3, 1, 1, 3), nrow = 2)\nx\nfisher.test(x)\n\nThis tests for an association between truth and guess, but if we want only positive associations, we should perform a one-sided test:\n\nfisher.test(x, alternative = \"greater\")\n\nA similar test can be performed with the chi-squared test, but this is not precise for small data sets, so the Fisher test should be preferred.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#discussion",
    "href": "qmd/06-classical-tests.html#discussion",
    "title": "06-Classical Tests",
    "section": "3.3 Discussion",
    "text": "3.3 Discussion\nExercise 5: Compare the results of Fisher’s exact test with alternative=\"two.sided\" (the default) with alternative = \"greater\" and less and discuss the differences and their meaning. Which option is the best in this case?\nExercise 6: How many trials would be necessary to get a significant statistical result with \\(p &lt; 0.05\\) for the tea taster experiment, given that we allow one wrong decision for tea first and milk first?",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#background",
    "href": "qmd/06-classical-tests.html#background",
    "title": "06-Classical Tests",
    "section": "3.4 Background",
    "text": "3.4 Background\nRead the Wikipedia articles “Lady tasting tea” about Fisher’s experiment and Fisher’s exact test and “The Lady Tasting Tea” about a popular science book on the “statistical revolution” in the 20th century.\nThe odds ratio describes the strength of association in a two-by-two table, see explanation of “Odds ratio” in Wikipedia or a statistics text book.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#references",
    "href": "qmd/06-classical-tests.html#references",
    "title": "06-Classical Tests",
    "section": "3.5 References",
    "text": "3.5 References",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe-project.html",
    "href": "qmd/02-discharge-elbe-project.html",
    "title": "x02-Discharge of River Elbe: Homework Project",
    "section": "",
    "text": "Interpret the results of Sections 3.3 and 3.4. Discuss which of the plots best shows the shape of the distribution of discharge data. You can include up to 4 plots. Add a table with the statistical parameters (min, max, \\(\\bar{x}, s_x\\), median, geometric mean and quartiles).\nThen discuss all results (parameters and figures) in connection.",
    "crumbs": [
      "Projects",
      "x02-Discharge of River Elbe: Homework Project"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe-project.html#communicate-in-your-team-with-other-teams-and-with-tutors",
    "href": "qmd/02-discharge-elbe-project.html#communicate-in-your-team-with-other-teams-and-with-tutors",
    "title": "x02-Discharge of River Elbe: Homework Project",
    "section": "3.1 Communicate in your team, with other teams and with tutors",
    "text": "3.1 Communicate in your team, with other teams and with tutors\nCommunicate results and ideas in your team and also with other teams and with the tutors. It is a good idea to speak (or email) with your collegues first, then collect material and create a first version (this is often longer as the 5 pages).\nThen discuss and improve the result, select the most important parts and create the final version.\nCommunication can be done in person or in the matrix. The idea of the matrix is that all (= “the community”) can learn from each other. Private communication channels for teamwork are of course allowed, just find an agreement what fits your needs best. If you want, it is also possible to create separate communication channels in Opal or the Matrix for the groups.\nIf you ask in the forum or the matrix, please formulate specific questions and not only “this is my result, is this correct”. Good examples will hopefully develop during the work. Please feel free to post questions and contribute to the answers.",
    "crumbs": [
      "Projects",
      "x02-Discharge of River Elbe: Homework Project"
    ]
  },
  {
    "objectID": "qmd/02-discharge-elbe-project.html#technical-recommendations",
    "href": "qmd/02-discharge-elbe-project.html#technical-recommendations",
    "title": "x02-Discharge of River Elbe: Homework Project",
    "section": "3.2 Technical recommendations",
    "text": "3.2 Technical recommendations\nThe report should read nicely. Space needed for figures, tables and explanatory text should be in good balance.\n\nThe report should have 5 pages at maximum. Quality instead of quantity! Distill the essential messages.\nFont size of the text should be 11 or 12 points.\nA line spacing of 1.2 lines is recommended to improve readability.\nFigures (lines, font size of annotations) must be well readable.\nLiterature must be properly cited. We encourage to use author-year style citations. Good examples can be found at the APA style web page.\n\nFinally, upload your result as PDF and (optionally) an R script to the File folder of your group. Submissions after the deadline cannot be considered.",
    "crumbs": [
      "Projects",
      "x02-Discharge of River Elbe: Homework Project"
    ]
  },
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "Elements of Applied Statistics – Lab Exercises",
    "section": "",
    "text": "This website contains a collection of lab-exercises for an introductory statistics course with R. The aim is to provide insight in fundamental principles and a broad overview and enable students to select and understand particular books and online material to dig in deeper in the diverse and fascinating field of statistics."
  },
  {
    "objectID": "00-index.html#introduction",
    "href": "00-index.html#introduction",
    "title": "Elements of Applied Statistics – Lab Exercises",
    "section": "",
    "text": "This website contains a collection of lab-exercises for an introductory statistics course with R. The aim is to provide insight in fundamental principles and a broad overview and enable students to select and understand particular books and online material to dig in deeper in the diverse and fascinating field of statistics."
  },
  {
    "objectID": "00-index.html#status",
    "href": "00-index.html#status",
    "title": "Elements of Applied Statistics – Lab Exercises",
    "section": "Status",
    "text": "Status\nThe selection of material is work in progress. Additional material will appear until end of January 2025. Feedback and comments are welcome."
  },
  {
    "objectID": "00-index.html#author",
    "href": "00-index.html#author",
    "title": "Elements of Applied Statistics – Lab Exercises",
    "section": "Author",
    "text": "Author\nhttps://tu-dresden.de/Members/thomas.petzoldt\nhttps://github.com/tpetzoldt\n2024-10-21"
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html",
    "href": "qmd/01-pivot-tables-with-libreoffice.html",
    "title": "01-Discharge of River Elbe: Data Management with LibreOffice",
    "section": "",
    "text": "Planning and maintenance of waterways and rivers needs adequate measurements and data. However, raw time series can often be long and confusing, so a first step is aggregation and visualization.\nScientific aim: plot an average year and calculate monthly averages, minima and maxima of the discharge.",
    "crumbs": [
      "Labs",
      "01-Discharge of River Elbe: Data Management with LibreOffice"
    ]
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html#download-the-data-set-and-inspect-the-data",
    "href": "qmd/01-pivot-tables-with-libreoffice.html#download-the-data-set-and-inspect-the-data",
    "title": "01-Discharge of River Elbe: Data Management with LibreOffice",
    "section": "3.1 Download the data set and inspect the data",
    "text": "3.1 Download the data set and inspect the data\n\nDownload the data set (elbe_data.ods) from the course home page and save it to a personal folder or your USB pendrive.\nOpen it with LibreOffice Calc. Excel has a similar functionality, but details differ.\nMake sure that you have set Calc to English language.\nInspect the data. You see that the date format has the form YYYY-MM-DD, that is the so-called “ISO 8601” date format, the international standard that makes data exchange between diferent software systems easier2.",
    "crumbs": [
      "Labs",
      "01-Discharge of River Elbe: Data Management with LibreOffice"
    ]
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html#creation-of-categories",
    "href": "qmd/01-pivot-tables-with-libreoffice.html#creation-of-categories",
    "title": "01-Discharge of River Elbe: Data Management with LibreOffice",
    "section": "3.2 Creation of categories",
    "text": "3.2 Creation of categories\nIn the following, we intend to aggregate the discharge data according to certain criteria, e.g. year and month. This can be done with the “pivot table” tool, so before we can do this, we need to create additional columns with the categories.\n\n3.2.1 Date computations\nCreate the following categorical columns using formulas for date computation:\n\nyear:= YEAR(A2)\nmonth: = MONTH(A2)\nday: = DAY(A2)\nweekday: = WEEKDAY(A2)\ndoy: = A2 - DATE(YEAR(A2), 1, 1) + 1\n\nThe last formula computes thew “day of year” (doy), also called Julian day. Here DATE(YEAR(A2), 1, 1) creates a date for the first January of the respective year and then the difference (+1) between the respective day and the corresponding 1st January. The formula respects the different length of months automatically, including 29th February in leap years.\nThen fill the formulas down the column until the end of the data column.\nNote: The formulas above assume that you use LibreOffice with English user interface. If you use another language (e.g. German) or other program (e.g. Excel), then the keywords and delimiters (semi-colon instead of comma) of the formulas may be different and you have to look up for them in the function library.\n\n\n3.2.2 Pivot tables\nIn LibreOffice pivot tables are created like follows:\n\nSelect the data range for which the pivot table is to be created (including header!),\nIn the menu select Insert, Pivot Table\nSelect Source – Current selection – ok\nNow drag the appropriate items to the fields, e.g. “year” to the Column field, “month” to Row field and “discharge” to Data field.\nDouble click on “discharge” and change Function to “Average”.\nOK\n\nThat’s it, and you get the monthly average discharge values.\nTask: Repeat the same for individual years, and for all years to extract minimum and maximum discharge and find a way to show the results graphically.\n\n\n3.2.3 Average year and seasonality\nCreate a pivot table with “year” as Column fields, “doy” as Row fields and “Mean discharge” (i.e. Average) as Data fields.\nPlot all years as function of the day of year.\nThen create the some of following plots (you may need additional pivot tables):\n\na bar chart for annual discharge sums (y=annual discharge, x=year)\na bar chart with average discharge for the 12 months.\nXY (Scatter) chart for all years like example before, and in addition average discharge for all observed years as thick line. Note: here it may be necessary to copy the numbers only from the pivot table to a separate sheet (Paste special – numbers) to remove the pivot table automatism for the graphics.\nXY (Scatter) chart with confidence band. Calculate maximum, average and minimum per doy over all years 3 lines: y = discharge min / average / max, x = doy.\n\nNow interpret the results. What was 2002? Google for “Elbe river 2002”.\n\n\n3.2.4 Cumulative sum plot3\nCumulative sum plots of rainfall, discharge or temperature are useful for reservoir managers, or to classify years whether they are dry, wet, cold or warm.\nCreate a cumulative sum plot for each year by adding the discharge data (\\(Q\\)) as follows:\n\\[\\begin{align*}\nQ_{sum, 1} &= Q_1 \\\\\nQ_{sum, 2} &= Q_1 + Q_2\\\\\nQ_{sum, 3} &= Q_1 + Q_2 + Q_3\\\\\nQ_{sum, n} &= Q_1 + Q_2 + Q_3 + \\cdots + Q_n\\\\\n\\end{align*}\\]\nAnswer the following questions. Which year was:\n\nthe wettest,\nthe driest,\nhad a wet winter and a dry summer?\n\n\n\n3.2.5 Additional ideas\nThe following ideas are intended as a stimulus for own explorations of the data and creative work. Feel free just to play around with downloaded data or develop your own project.\n\nRepeat the analysis with an additional elbe.csv4 file with more years.\nDownload data from other measurement stations, e.g.\n\nfrom http://www.fgg-elbe.de/elbe-datenportal.html or\nU.S. Geological Survey http://waterdata.usgs.gov/\n\n\nand analyse the data.",
    "crumbs": [
      "Labs",
      "01-Discharge of River Elbe: Data Management with LibreOffice"
    ]
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html#footnotes",
    "href": "qmd/01-pivot-tables-with-libreoffice.html#footnotes",
    "title": "01-Discharge of River Elbe: Data Management with LibreOffice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData Source: Federal Waterways and Shipping Administration (WSV), provided by the Federal Institute for Hydrology (BfG).↩︎\nFor details, see http://en.wikipedia.org/wiki/ISO_dates↩︎\noptional topic with higher difficulty↩︎\nhttps://github.com/tpetzoldt/datasets/blob/main/data/↩︎",
    "crumbs": [
      "Labs",
      "01-Discharge of River Elbe: Data Management with LibreOffice"
    ]
  },
  {
    "objectID": "qmd/13-timeseries-breakpoints.html",
    "href": "qmd/13-timeseries-breakpoints.html",
    "title": "13-Identification of Breakpoints in Time Series",
    "section": "",
    "text": "The example is adapted from the help pages of R package “strucchange”, see Zeileis, A. et al. (2002), Journal of Statistical Software, 7(2), https://www.jstatsoft.org/v07/i02/\n\nlibrary(strucchange)\n\nLade nötiges Paket: zoo\n\n\n\nAttache Paket: 'zoo'\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLade nötiges Paket: sandwich\n\ndata(\"Nile\")\nplot(Nile)\n\n\n\n\n\n\n\n## OLS-CUMSUM test for structural breaks in the time series\n## are there periods with different discharge?\nocus &lt;- efp(Nile ~ 1, type = \"OLS-CUSUM\")\nplot(ocus)\n\n\n\n\n\n\n\nsctest(ocus)\n\n\n    OLS-based CUSUM test\n\ndata:  ocus\nS0 = 2.9518, p-value = 5.409e-08\n\n## identify time of structural break (with respect to mean value)\nbp.nile &lt;- breakpoints(Nile ~ 1)\nsummary(bp.nile)\n\n\n     Optimal (m+1)-segment partition: \n\nCall:\nbreakpoints.formula(formula = Nile ~ 1)\n\nBreakpoints at observation number:\n                      \nm = 1      28         \nm = 2      28       83\nm = 3      28    68 83\nm = 4      28 45 68 83\nm = 5   15 30 45 68 83\n\nCorresponding to breakdates:\n                                \nm = 1        1898               \nm = 2        1898           1953\nm = 3        1898      1938 1953\nm = 4        1898 1915 1938 1953\nm = 5   1885 1900 1915 1938 1953\n\nFit:\n                                                   \nm   0       1       2       3       4       5      \nRSS 2835157 1597457 1552924 1538097 1507888 1659994\nBIC    1318    1270    1276    1285    1292    1311\n\n## the BIC also chooses one breakpoint\nplot(bp.nile)\n\n\n\n\n\n\n\n## fit null hypothesis model and model with 1 breakpoint\nfm0 &lt;- lm(Nile ~ 1)\nfm1 &lt;- lm(Nile ~ breakfactor(bp.nile,  breaks = 1))\nplot(Nile)\nlines(ts(fitted(fm0),  start = 1871),  col = 3)\nlines(ts(fitted(fm1),  start = 1871),  col = 4)\nlines(bp.nile)\n\n## confidence interval\nci.nile &lt;- confint(bp.nile)\nci.nile\n\n\n     Confidence intervals for breakpoints\n     of optimal 2-segment partition: \n\nCall:\nconfint.breakpointsfull(object = bp.nile)\n\nBreakpoints at observation number:\n  2.5 % breakpoints 97.5 %\n1    25          28     32\n\nCorresponding to breakdates:\n  2.5 % breakpoints 97.5 %\n1  1895        1898   1902\n\nlines(ci.nile)\n\n\n\n\n\n\n\n## mark breakoint using a simpler (and less fancy) method\nplot(Nile)\ndat &lt;- data.frame(time = time(Nile), Q = as.vector(Nile))\nabline(v=dat$time[bp.nile$breakpoints],  col=\"green\")\n\n\n\n\n\n\n\n## ANOVA test whether the two models are significantly different\nanova(fm0, fm1)\n\nAnalysis of Variance Table\n\nModel 1: Nile ~ 1\nModel 2: Nile ~ breakfactor(bp.nile, breaks = 1)\n  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     99 2835157                                 \n2     98 1597457  1   1237700 75.93 7.439e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## alternative: AIC-based model comparison. \n## The model with lower AIC is better\nAIC(fm0,fm1)\n\n    df      AIC\nfm0  2 1313.031\nfm1  3 1257.663\n\n## some tests for quality and assumptions of the fitted model\nacf(residuals(fm0))\n\n\n\n\n\n\n\nacf(residuals(fm1))\n\n\n\n\n\n\n\nqqnorm(residuals(fm0))\n\n\n\n\n\n\n\nqqnorm(residuals(fm1))",
    "crumbs": [
      "Labs",
      "13-Identification of Breakpoints in Time Series"
    ]
  },
  {
    "objectID": "qmd/13-timeseries-breakpoints.html#text-needs-to-be-written",
    "href": "qmd/13-timeseries-breakpoints.html#text-needs-to-be-written",
    "title": "13-Identification of Breakpoints in Time Series",
    "section": "",
    "text": "The example is adapted from the help pages of R package “strucchange”, see Zeileis, A. et al. (2002), Journal of Statistical Software, 7(2), https://www.jstatsoft.org/v07/i02/\n\nlibrary(strucchange)\n\nLade nötiges Paket: zoo\n\n\n\nAttache Paket: 'zoo'\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLade nötiges Paket: sandwich\n\ndata(\"Nile\")\nplot(Nile)\n\n\n\n\n\n\n\n## OLS-CUMSUM test for structural breaks in the time series\n## are there periods with different discharge?\nocus &lt;- efp(Nile ~ 1, type = \"OLS-CUSUM\")\nplot(ocus)\n\n\n\n\n\n\n\nsctest(ocus)\n\n\n    OLS-based CUSUM test\n\ndata:  ocus\nS0 = 2.9518, p-value = 5.409e-08\n\n## identify time of structural break (with respect to mean value)\nbp.nile &lt;- breakpoints(Nile ~ 1)\nsummary(bp.nile)\n\n\n     Optimal (m+1)-segment partition: \n\nCall:\nbreakpoints.formula(formula = Nile ~ 1)\n\nBreakpoints at observation number:\n                      \nm = 1      28         \nm = 2      28       83\nm = 3      28    68 83\nm = 4      28 45 68 83\nm = 5   15 30 45 68 83\n\nCorresponding to breakdates:\n                                \nm = 1        1898               \nm = 2        1898           1953\nm = 3        1898      1938 1953\nm = 4        1898 1915 1938 1953\nm = 5   1885 1900 1915 1938 1953\n\nFit:\n                                                   \nm   0       1       2       3       4       5      \nRSS 2835157 1597457 1552924 1538097 1507888 1659994\nBIC    1318    1270    1276    1285    1292    1311\n\n## the BIC also chooses one breakpoint\nplot(bp.nile)\n\n\n\n\n\n\n\n## fit null hypothesis model and model with 1 breakpoint\nfm0 &lt;- lm(Nile ~ 1)\nfm1 &lt;- lm(Nile ~ breakfactor(bp.nile,  breaks = 1))\nplot(Nile)\nlines(ts(fitted(fm0),  start = 1871),  col = 3)\nlines(ts(fitted(fm1),  start = 1871),  col = 4)\nlines(bp.nile)\n\n## confidence interval\nci.nile &lt;- confint(bp.nile)\nci.nile\n\n\n     Confidence intervals for breakpoints\n     of optimal 2-segment partition: \n\nCall:\nconfint.breakpointsfull(object = bp.nile)\n\nBreakpoints at observation number:\n  2.5 % breakpoints 97.5 %\n1    25          28     32\n\nCorresponding to breakdates:\n  2.5 % breakpoints 97.5 %\n1  1895        1898   1902\n\nlines(ci.nile)\n\n\n\n\n\n\n\n## mark breakoint using a simpler (and less fancy) method\nplot(Nile)\ndat &lt;- data.frame(time = time(Nile), Q = as.vector(Nile))\nabline(v=dat$time[bp.nile$breakpoints],  col=\"green\")\n\n\n\n\n\n\n\n## ANOVA test whether the two models are significantly different\nanova(fm0, fm1)\n\nAnalysis of Variance Table\n\nModel 1: Nile ~ 1\nModel 2: Nile ~ breakfactor(bp.nile, breaks = 1)\n  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     99 2835157                                 \n2     98 1597457  1   1237700 75.93 7.439e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## alternative: AIC-based model comparison. \n## The model with lower AIC is better\nAIC(fm0,fm1)\n\n    df      AIC\nfm0  2 1313.031\nfm1  3 1257.663\n\n## some tests for quality and assumptions of the fitted model\nacf(residuals(fm0))\n\n\n\n\n\n\n\nacf(residuals(fm1))\n\n\n\n\n\n\n\nqqnorm(residuals(fm0))\n\n\n\n\n\n\n\nqqnorm(residuals(fm1))",
    "crumbs": [
      "Labs",
      "13-Identification of Breakpoints in Time Series"
    ]
  },
  {
    "objectID": "qmd/07-correlation.html",
    "href": "qmd/07-correlation.html",
    "title": "07-Correlation",
    "section": "",
    "text": "Given are the following series of measurements from a polluted river (Ertel et al., 2012):\n\nStation &lt;- 1:17\nCOD   &lt;- c(9.6, 12.2, 13.2, 13.3, 37.4, 21.4, 16.1, 24.8, 24.2, 26.9,\n           29.2, 31.6, 18.2, 24.8, 13.7, 23.6, 24.2)\nO2    &lt;- c(9.8, 10, 8.3, 9.6, 1.5, 4.4, 6.3, 3, 3, 10, 9.4, 17.9, 9.7,\n           9.7, 8.2, 9.5, 11.3)\nNH4   &lt;- c(0.15, 0.1, 0.74, 0.29, 5.04, 2.26, 0.96, 3.37, 2.44, 0.27,\n          0.32, 0.68, 0.27, 0.32, 0.22, 0.58, 0.59)\nColor &lt;- c(0.59, 0.52, 0.6, 0.57, 1.34, 1.21, 1.17, 1.12, 1.1, 1.08,\n           1.24, 1.25, 1.29, 1.29, 1.06, 1.25, 1.16)\n\nwith COD, the chemical oxygen demand (in \\(\\mathrm{mg L^{-1}}\\)), \\(\\mathrm O_2\\) oxygen concentration (\\(\\mathrm{mg L^{-1}}\\)) and Color. The spectral absorption coefficient at 436nm (\\(\\mathrm m^{-1}\\)) is a measure of the color intensity of the filtered water. The data set is subset from field measurements, that contained more samples from several measurement campaigns.\nThe question is whether Ammonium (NH4), Oxygen (O2) and Color depend on the organic load (COD).\n\n\n\nFor the dependency between Color and COD we can use Pearson correlation. In addition it is always a good idea to plot the data.\n\nplot(COD, Color)\ncor.test(COD, Color)\n\nWe see an almost linear dependency and get a highly significant correlation. Now, for the dependence between Ammonium on COD we can proceed with:\n\nplot(COD, NH4)\ncor.test(COD, NH4)\n\nWe find again significant correlation. However, the dependency is not very strict and it seems that there are two different data sets. This is in fact true because sampling sites 1–9 were before and the other sampling sites below a cooling reservoir of a power plant. In the following, we create first an empty plot(...., type = \"n\") and then add the station number as text labels:\n\nplot(COD, NH4, type = \"n\")\ntext(COD, NH4, labels = Station)\n\nWe now repeat the analysis with the first 9 data pairs from the stations upstream of the cooling reservoir (50.19N, 24.40E, Link to Google Maps):\n\nplot(COD[1:9], NH4[1:9])\ncor.test(COD[1:9], NH4[1:9])\n\n\n\n\nIs oxygen concentration (O2) directly related to organic pollution (COD)? Interpret the results, and discuss the potential mechanisms how the cooling reservoir may influence water quality. Compare your conclusions with the paper of (Ertel et al. 2011). Does the assumption of independent residuals hold?\nRepeat the analysis with Spearman’s correlation, and compare the results with the Pearson correlation coefficients.",
    "crumbs": [
      "Labs",
      "07-Correlation"
    ]
  },
  {
    "objectID": "qmd/07-correlation.html#introduction",
    "href": "qmd/07-correlation.html#introduction",
    "title": "07-Correlation",
    "section": "",
    "text": "Given are the following series of measurements from a polluted river (Ertel et al., 2012):\n\nStation &lt;- 1:17\nCOD   &lt;- c(9.6, 12.2, 13.2, 13.3, 37.4, 21.4, 16.1, 24.8, 24.2, 26.9,\n           29.2, 31.6, 18.2, 24.8, 13.7, 23.6, 24.2)\nO2    &lt;- c(9.8, 10, 8.3, 9.6, 1.5, 4.4, 6.3, 3, 3, 10, 9.4, 17.9, 9.7,\n           9.7, 8.2, 9.5, 11.3)\nNH4   &lt;- c(0.15, 0.1, 0.74, 0.29, 5.04, 2.26, 0.96, 3.37, 2.44, 0.27,\n          0.32, 0.68, 0.27, 0.32, 0.22, 0.58, 0.59)\nColor &lt;- c(0.59, 0.52, 0.6, 0.57, 1.34, 1.21, 1.17, 1.12, 1.1, 1.08,\n           1.24, 1.25, 1.29, 1.29, 1.06, 1.25, 1.16)\n\nwith COD, the chemical oxygen demand (in \\(\\mathrm{mg L^{-1}}\\)), \\(\\mathrm O_2\\) oxygen concentration (\\(\\mathrm{mg L^{-1}}\\)) and Color. The spectral absorption coefficient at 436nm (\\(\\mathrm m^{-1}\\)) is a measure of the color intensity of the filtered water. The data set is subset from field measurements, that contained more samples from several measurement campaigns.\nThe question is whether Ammonium (NH4), Oxygen (O2) and Color depend on the organic load (COD).",
    "crumbs": [
      "Labs",
      "07-Correlation"
    ]
  },
  {
    "objectID": "qmd/07-correlation.html#data-analysis",
    "href": "qmd/07-correlation.html#data-analysis",
    "title": "07-Correlation",
    "section": "",
    "text": "For the dependency between Color and COD we can use Pearson correlation. In addition it is always a good idea to plot the data.\n\nplot(COD, Color)\ncor.test(COD, Color)\n\nWe see an almost linear dependency and get a highly significant correlation. Now, for the dependence between Ammonium on COD we can proceed with:\n\nplot(COD, NH4)\ncor.test(COD, NH4)\n\nWe find again significant correlation. However, the dependency is not very strict and it seems that there are two different data sets. This is in fact true because sampling sites 1–9 were before and the other sampling sites below a cooling reservoir of a power plant. In the following, we create first an empty plot(...., type = \"n\") and then add the station number as text labels:\n\nplot(COD, NH4, type = \"n\")\ntext(COD, NH4, labels = Station)\n\nWe now repeat the analysis with the first 9 data pairs from the stations upstream of the cooling reservoir (50.19N, 24.40E, Link to Google Maps):\n\nplot(COD[1:9], NH4[1:9])\ncor.test(COD[1:9], NH4[1:9])",
    "crumbs": [
      "Labs",
      "07-Correlation"
    ]
  },
  {
    "objectID": "qmd/07-correlation.html#exercises-and-discussion",
    "href": "qmd/07-correlation.html#exercises-and-discussion",
    "title": "07-Correlation",
    "section": "",
    "text": "Is oxygen concentration (O2) directly related to organic pollution (COD)? Interpret the results, and discuss the potential mechanisms how the cooling reservoir may influence water quality. Compare your conclusions with the paper of (Ertel et al. 2011). Does the assumption of independent residuals hold?\nRepeat the analysis with Spearman’s correlation, and compare the results with the Pearson correlation coefficients.",
    "crumbs": [
      "Labs",
      "07-Correlation"
    ]
  },
  {
    "objectID": "qmd/07-correlation.html#introduction-1",
    "href": "qmd/07-correlation.html#introduction-1",
    "title": "07-Correlation",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nGiven is a number of students that passed an examination in statistics. The examination was written two times, one time before and one time after an additional series of lectures. The values represent the number of points approached during the examination. Check whether there is a dependency between the results before and after the test, i.e. if there is any dependency between the results of the final test and the basic knowledge before the course.",
    "crumbs": [
      "Labs",
      "07-Correlation"
    ]
  },
  {
    "objectID": "qmd/07-correlation.html#data-and-data-analysis",
    "href": "qmd/07-correlation.html#data-and-data-analysis",
    "title": "07-Correlation",
    "section": "2.2 Data and data analysis",
    "text": "2.2 Data and data analysis\n\nx1 &lt;- c(69, 77, 35, 34, 87, 45, 95, 83)\nx2 &lt;- c(100, 97, 67, 42, 75, 73, 92, 97)\ncor.test(x1, x2)\n\nIf we plot this, we see a rather strange pattern, i.e. no clear linear relationship:\n\nplot(x1, x2)\n\nTherefore it may be a good idea to use the rank correlation:\n\ncor.test(x1, x2, method=\"spearman\")\n\nSometimes, we may get a warning that it “cannot compute exact p-values with ties”, then we can use another approach and compute the Spearman correlation via the Pearson correlation of ranks:\n\ncor.test(rank(x1), rank(x2))\n\nThis needs a little bit more effort (for the computer, not for us), but the interpretation is the same. To understand how this worked, it can be a good idea to create a scatterplot of the ranks of both variables.",
    "crumbs": [
      "Labs",
      "07-Correlation"
    ]
  },
  {
    "objectID": "qmd/07-correlation.html#exercise-and-discussion",
    "href": "qmd/07-correlation.html#exercise-and-discussion",
    "title": "07-Correlation",
    "section": "2.3 Exercise and Discussion",
    "text": "2.3 Exercise and Discussion\nWhat do the results above tell us? Compare the results with a paired t-test of the same data set. Which test tells what?",
    "crumbs": [
      "Labs",
      "07-Correlation"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html",
    "href": "qmd/10-nonlinear-regression-solution.html",
    "title": "x10-Fit nonlinear model to plankton growth data",
    "section": "",
    "text": "The growth rate of a population is a direct measure of fitness. Therefore, determination of growth rates is common in many disciplines of natural and human sciences, business and engineering: ecology, pharmacology, wastewater treatment, and economic growth. The following example gives a brief introduction, how growth models can be fitted wit R.",
    "crumbs": [
      "Solutions",
      "x10-Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#data-set",
    "href": "qmd/10-nonlinear-regression-solution.html#data-set",
    "title": "x10-Fit nonlinear model to plankton growth data",
    "section": "2.1 Data set",
    "text": "2.1 Data set\nThe example data set was taken from a growth experiment in a batch culture with Microcystis aeruginosa, a cyanobacteria (blue green algae) species. Details of the experiment can be found in Jähnichen et al. (2001).\n\n## time (t)\nx &lt;- c(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20)\n## Algae cell counts (per ml)\ny &lt;- c(0.88, 1.02, 1.43, 2.79, 4.61, 7.12,\n       6.47, 8.16, 7.28, 5.67, 6.91) * 1e6",
    "crumbs": [
      "Solutions",
      "x10-Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#methods",
    "href": "qmd/10-nonlinear-regression-solution.html#methods",
    "title": "x10-Fit nonlinear model to plankton growth data",
    "section": "2.2 Methods",
    "text": "2.2 Methods\nParametric models are fitted using nonlinear regression according to the method of least squares. Data analysis is performed using the R software of statistical computing and graphics (R Core Team, 2021) and the nls function from package stats. An additional analysis is performed with packages growthrates (Petzoldt, 2020) and FME (Soetaert & Petzoldt, 2010).\nTo get a suitable curve, we need a model that fits the data and that has identifiable parameters. In the following, we use the logistic growth model (Verhulst, 1838):\n\\[\nN = \\frac{K \\cdot N_0}{(N_0 + (K - N_0) \\cdot \\exp(-r \\cdot x))}\n\\]\nand the Baranyi-Roberts model (Baranyi & Roberts, 1994), explained later.",
    "crumbs": [
      "Solutions",
      "x10-Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#nonlinear-regression-with-nls",
    "href": "qmd/10-nonlinear-regression-solution.html#nonlinear-regression-with-nls",
    "title": "x10-Fit nonlinear model to plankton growth data",
    "section": "3.1 Nonlinear regression with “nls”",
    "text": "3.1 Nonlinear regression with “nls”\n\n3.1.1 Logistic Growth\nWe define now a used defined function for the logistic and this by plotting the function with the start values (blue line). Then we can use function nls (nonlinear least squares) to fit the model:\n\n## function definition\nf &lt;- function(x, r, K, N0) {K /(1 + (K/N0 - 1) * exp(-r *x))}\n\n## check of start values\nplot(x, yy, pch=16, xlab=\"time (days)\", ylab=\"algae (Mio cells)\")\nlines(x, f(x, r=r, K=max(yy), N0=yy[1]), col=\"blue\")\n\n## nonlinear regression\npstart &lt;- c(r=r, K=max(yy), N0=yy[1])\nfit_logistic   &lt;- nls(yy ~ f(x, r, K, N0), start = pstart, trace=FALSE)\n\nx1 &lt;- seq(0, 25, length = 100)\nlines(x1, predict(fit_logistic, data.frame(x = x1)), col = \"red\")\nlegend(\"topleft\",\n       legend = c(\"data\", \"start parameters\", \"fitted parameters\"),\n       col = c(\"black\", \"blue\", \"red\"),\n       lty = c(0, 1, 1),\n       pch = c(16, NA, NA))\n\n\n\n\n\n\n\nsummary(fit_logistic)\n\n\nFormula: yy ~ f(x, r, K, N0)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nr    0.5682     0.1686   3.371  0.00978 ** \nK    7.0725     0.4033  17.535 1.14e-07 ***\nN0   0.1757     0.1861   0.944  0.37271    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8118 on 8 degrees of freedom\n\nNumber of iterations to convergence: 14 \nAchieved convergence tolerance: 4.018e-06\n\n(Rsquared &lt;- 1 - var(residuals(fit_logistic))/var(yy))\n\n[1] 0.931732\n\n\nWe see that the fit converged and the red line approximates the data, but we can also see that the model fit is far below the data at the beginning. This will be improved in the next section.\n\n\n3.1.2 Baranyi-Roberts model\nThe logistic function assumes, that growth starts exponentially from the beginning and then approaches more and more saturation. In reality, organisms need often some time to adapt to new conditions, and we can observe a delay at the beginnig. This delay is called lag-phase. Several models exist to describe such behavior, where the Baranyi-Roberts model (Baranyi & Roberts, 1994) is one of the most commonly used. Its parameters are similar to the logistic function with one additional parameter \\(h_0\\) for the lag. Following its mathematical equation (not shown here), we can implement it a suser-defined function in R:\n\nbaranyi &lt;- function(x, r, K, N0, h0) {\n  A &lt;- x + 1/r * log(exp(-r * x) + exp(-h0) - exp(-r * x - h0))\n  y &lt;- exp(log(N0) + r * A - log(1 + (exp(r * A) - 1)/exp(log(K) - log(N0))))\n  y\n}\n\nIf we assume a lag time \\(h_0 = 2\\), we can try to fit it and compare it with the logistic model\n\npstart &lt;- c(r=0.5, K=7, N0=1, h0=2)\nfit_baranyi   &lt;- nls(yy ~ baranyi(x, r, K, N0, h0), start = pstart, trace=FALSE)\n\nplot(x, yy, pch=16, xlab=\"time (days)\", ylab=\"algae (Mio cells)\")\nlines(x1, predict(fit_logistic, data.frame(x = x1)), col = \"red\")\nlines(x1, predict(fit_baranyi, data.frame(x = x1)), col = \"forestgreen\", lwd=2)\n\nlegend(\"topleft\",\n       legend = c(\"data\", \"logistic model\", \"Baranyi-Roberts model\"),\n       col = c(\"black\", \"red\", \"forestgreen\"),\n       lty = c(0, 1, 1),\n       pch = c(16, NA, NA))\n\n\n\n\n\n\n\n\nIt is obvious, that it fits much better.",
    "crumbs": [
      "Solutions",
      "x10-Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#growth-curve-fitting-with-r-package-growthrates",
    "href": "qmd/10-nonlinear-regression-solution.html#growth-curve-fitting-with-r-package-growthrates",
    "title": "x10-Fit nonlinear model to plankton growth data",
    "section": "3.2 Growth curve fitting with R package “growthrates”",
    "text": "3.2 Growth curve fitting with R package “growthrates”\nAs growth curves are of fundamental importance in science and engineering, several R packages exist for this problem. Here we show one of these packages growthrates (Petzoldt, 2020). Details can be found in the package documentation.\n\n3.2.1 Maximum growth rate as steepest increase in log scale\nThe package contains a method “easy linear” to find the steepest linear increase. It is a fully automatic method employing linear regression and a search routine. Details of the algorithm are found in Hall et al. (2014).\nThe following shows the phase of steepest increase, the exponential phase, identified by linear regression using the data points with the steepest increase:\n\nlibrary(\"growthrates\")\npar(mfrow=c(1, 2))\nfit_easy &lt;- fit_easylinear(x, yy)\nplot(fit_easy, main=\"linear scale\")\nplot(fit_easy, log=\"y\", main=\"log scale\")\n\n\n\n\n\n\n\ncoef(fit_easy)\n\n       y0     y0_lm     mumax       lag \n0.8800000 0.5838576 0.2528382 1.6226381 \n\n\n\n\n3.2.2 Logistic growth\nNow we can take the start parameters from above and function fit_growthmodel using the grow_logistic function, that is pre-defined in the package. We can also use a specific plot function from the package\n\npstart &lt;- c(mumax=r, K=max(yy), y0=yy[1])\nfit_logistic2 &lt;- fit_growthmodel(grow_logistic, p=pstart, time=x, y=yy)\nplot(fit_logistic2)\n\n\n\n\n\n\n\n\n\n\n3.2.3 Baranyi-Roberts model\nWe see again that the model fits not very well at the beginning because of the lag phase. Therefore, we empoy again an extended model e.g. the Baranyi model.\nA start value for the lag phase parameter \\(h_0\\) can be approximated from the “easylinear” method:\n\ncoef(fit_easy)\n\n       y0     y0_lm     mumax       lag \n0.8800000 0.5838576 0.2528382 1.6226381 \n\nh0 &lt;- 0.25 * 1.66\n\npstart &lt;- c(mumax=0.5, K=max(yy), y0=yy[1], h0=h0)\nfit_baranyi2 &lt;- fit_growthmodel(grow_baranyi, p=pstart, time=x, y=yy)\nsummary(fit_baranyi2)\n\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nmumax   0.8477     0.3681   2.303   0.0547 .  \nK       6.9969     0.3499  19.999 1.96e-07 ***\ny0      0.9851     0.5250   1.876   0.1027    \nh0      4.1220     3.0894   1.334   0.2239    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7583 on 7 degrees of freedom\n\nParameter correlation:\n        mumax       K      y0      h0\nmumax  1.0000 -0.3607  0.4600  0.9635\nK     -0.3607  1.0000 -0.1030 -0.2959\ny0     0.4600 -0.1030  1.0000  0.6477\nh0     0.9635 -0.2959  0.6477  1.0000\n\n\nThe summary shows the parameter estimates, their standard error and a significance level. However, we should not take the significance stars too seriously here. If we would, for example, omit the “nonsignificant” parameters y0 and h0, or set it to zero, the models would not work anymore. We see that some parameters correlate, especially h0 and y0. This can, in principle, indicate identification problems, but this dod not happen here, fortunatly.\nFinally, we plot the results in both, linear and log scale:\n\npar(mfrow=c(1, 2))\nplot(fit_logistic2, ylim=c(0, 10), las=1)\nlines(fit_baranyi2, col=\"magenta\")\n\npoints(x, yy, pch=16, col=\"red\")\n\n## log scale\nplot(fit_logistic2, log=\"y\", ylim=c(0.2, 10), las=1)\npoints(x, yy, pch=16, col=\"red\")\nlines(fit_baranyi2, col=\"magenta\")\nlegend(\"bottomright\",\n       legend = c(\"data\", \"logistic model\", \"Baranyi model\"),\n       col = c(\"red\", \"blue\", \"magenta\"),\n       lty = c(0, 1, 1),\n       pch = c(16, NA, NA))",
    "crumbs": [
      "Solutions",
      "x10-Fit nonlinear model to plankton growth data"
    ]
  },
  {
    "objectID": "qmd/14-flood-risk.html",
    "href": "qmd/14-flood-risk.html",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "",
    "text": "Here, it is described how to fit distributions to a given hydrollogical data set. Our intention here is to provide an example how easy and powerful distribution fitting can be done in R. More information can be found in Rice(2003), Hogg(2004), Coles (2001) and in the help file of package FAmle (Aucoin, 2001). For the given example, a data set from the US Geological Survey (USGS, http://waterdata.usgs.gov/nwis will be employed. The dataset consists of annual maximum daily peakflows (ft3/s) that were observed at a hydrometric station located at River James (Columbia). First the packages and the data set is loaded, then it is tested for potential trends and autocorrelation\n\n## load required packages\nlibrary(\"FAmle\")\nlibrary(\"FAdist\")\nlibrary(\"MASS\")\nlibrary(\"zoo\")\nlibrary(\"readr\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\n\n\n## St James River, Columbia\njamesriver &lt;- read_csv(\"jamesriver.csv\", col_types = c(\"D\", \"n\"))\n\nflow &lt;- jamesriver$flow\n\npar(mfrow=c(1, 2))\nplot(jamesriver$date, jamesriver$flow, type=\"b\", cex=0.4, pch=19, cex.axis=0.75, xlab=\"Year\", ylab=\"Flow\",\nmain=\"James River\")\nlines(lowess(jamesriver), col=\"red\")\nacf(jamesriver$flow, main=\"\")\n\n\n\n\n\n\n\nFigure 1: Time series (left) and auto-correlation plot (right) the daily flow (in ft3/s data set. The red smoothed line corresponds to a lowess fit.",
    "crumbs": [
      "Labs",
      "14-Extreme Value Estimation with Package FAmle"
    ]
  },
  {
    "objectID": "qmd/14-flood-risk.html#introduction",
    "href": "qmd/14-flood-risk.html#introduction",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "",
    "text": "Here, it is described how to fit distributions to a given hydrollogical data set. Our intention here is to provide an example how easy and powerful distribution fitting can be done in R. More information can be found in Rice(2003), Hogg(2004), Coles (2001) and in the help file of package FAmle (Aucoin, 2001). For the given example, a data set from the US Geological Survey (USGS, http://waterdata.usgs.gov/nwis will be employed. The dataset consists of annual maximum daily peakflows (ft3/s) that were observed at a hydrometric station located at River James (Columbia). First the packages and the data set is loaded, then it is tested for potential trends and autocorrelation\n\n## load required packages\nlibrary(\"FAmle\")\nlibrary(\"FAdist\")\nlibrary(\"MASS\")\nlibrary(\"zoo\")\nlibrary(\"readr\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\n\n\n## St James River, Columbia\njamesriver &lt;- read_csv(\"jamesriver.csv\", col_types = c(\"D\", \"n\"))\n\nflow &lt;- jamesriver$flow\n\npar(mfrow=c(1, 2))\nplot(jamesriver$date, jamesriver$flow, type=\"b\", cex=0.4, pch=19, cex.axis=0.75, xlab=\"Year\", ylab=\"Flow\",\nmain=\"James River\")\nlines(lowess(jamesriver), col=\"red\")\nacf(jamesriver$flow, main=\"\")\n\n\n\n\n\n\n\nFigure 1: Time series (left) and auto-correlation plot (right) the daily flow (in ft3/s data set. The red smoothed line corresponds to a lowess fit.",
    "crumbs": [
      "Labs",
      "14-Extreme Value Estimation with Package FAmle"
    ]
  },
  {
    "objectID": "qmd/14-flood-risk.html#empirical-quantiles",
    "href": "qmd/14-flood-risk.html#empirical-quantiles",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "2 Empirical Quantiles",
    "text": "2 Empirical Quantiles\n\nhist(flow, probability=TRUE, xlab=\"flow (ft^3/s)\")\nrug(flow)\nlines(density(flow))\n\n\n\n\n\n\n\nFigure 2: Histogram and empirical density of peak discharge.\n\n\n\n\n\nIf the data series is long enough, one may be tempted to use empirical quantiles, i.e. model and parameter free extrapolation from the data. We use this value as a baseline for the comparison with the model derived quantiles:\n\nquantile(p=c(0.95, 0.99), flow)\n\n 95%  99% \n4589 6974",
    "crumbs": [
      "Labs",
      "14-Extreme Value Estimation with Package FAmle"
    ]
  },
  {
    "objectID": "qmd/14-flood-risk.html#lognormal-distribution-with-2-parameters",
    "href": "qmd/14-flood-risk.html#lognormal-distribution-with-2-parameters",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "3 Lognormal Distribution with 2 Parameters",
    "text": "3 Lognormal Distribution with 2 Parameters\nThe Lognormal distribution is often regarded as a plausible model for this type of data. However, other distributions such as Weibull, Lognormal with three parameters, and Johnson distributions may provide better fitting results. We will try some of them. The parameters of the distribution are estimated using maximum likelihood by the mle function con tained in package “FAmle”, except for the Johnson distribution wich needs a different procedure. Parameters of the fitting can be obtained as follows. It is important to pay attention to goodness-of-fit parameters (log likelihood and AIC) which provide us information about how good the model explains the corresponding data set.\n\nfitLn2 &lt;- mle(x=flow, dist=\"lnorm\", start=c(0.1, 0.1))\nfitLn2\n\n-----------------------------------------\n       Maximum Likelihood Estimates\n-----------------------------------------\nData object:  flow \nDistribution:  lnorm \n\n--------- Parameter estimates -----------\n\n         meanlog.hat sdlog.hat\nEstimate       6.294    1.4878\nStd.err        0.186    0.1319\n\n---------- Goodness-of-Fit --------------\n\n log.like       aic        ad       rho \n-518.8617 1041.7234    0.9627    0.9884 \n-----------------------------------------\n\n\n\n## automatic diagnostic plots\nplot(x=fitLn2, ci=TRUE, alpha=0.05)\n\n## which probability has a flow &gt;= 3000\n##  --&gt; two functions to provide the same result:\n\n### standard R function\nplnorm(3000, meanlog=fitLn2$par.hat[1], sdlog=fitLn2$par.hat[2])\n\n[1] 0.8751862\n\n### function from the FAmle package\ndistr(x=3000, dist=\"lnorm\", param=c(fitLn2$par.hat[1], fitLn2$par.hat[2]), type=\"p\")\n\n[1] 0.8751862\n\n## same for quantile (flow &gt;= 95% quantile)\nqlnorm(p=0.95, meanlog=fitLn2$par.hat[1], sdlog=fitLn2$par.hat[2])\n\n[1] 6252.526\n\ndistr(x=0.95, dist=\"lnorm\", param=c(fitLn2$par.hat[1], fitLn2$par.hat[2]), type=\"q\")\n\n[1] 6252.526\n\n## empirical quantile\nquantile(p=0.95, flow)\n\n 95% \n4589 \n\n\n\n\n\n\n\n\nFigure 3: Plot of the mle object corresponding to the fitting James River data using a Lognormal distribution\n\n\n\n\n\nThe function mle() provides also some goodness-of-fit statistics. This function creates a special kind of object which can be used inside of the standard R functions, e.g., plot(). A function called plot.mle may be used to generate a series of four diagnosis plots (Figure 3) for the mle object. Diagnostic plots for the model fitted to the dataset. The dashed red lines correspond to the lower and upper confidence bounds (definded by alpha) of the approximated 95% confidence intervals derived using the observed Fisher’s information matrix in conjunction with the so-called delta method.\nOnce the function is fitted to a distribution, these parameters can be used to calculate different quan- tiles. In this way we can find, for example, the value of the flow which has a probability lower than 5% or which is the probability of a flooding event of a certain flow.\nNow repeat for the 99% quantile\n…\nAnd extreme floods: 1% quantile\n…\nThe probability of a peakflow of 3000 ft3/s is obtained by either function “plnorm” or “distr” like follows:\n\nplnorm(3000, meanlog=fitLn2$par.hat[1], sdlog=fitLn2$par.hat[2], lower.tail=TRUE)\n\n[1] 0.8751862\n\ndistr(x=3000, dist=\"lnorm\", param=fitLn2$par.hat, type=\"p\")\n\n[1] 0.8751862",
    "crumbs": [
      "Labs",
      "14-Extreme Value Estimation with Package FAmle"
    ]
  },
  {
    "objectID": "qmd/14-flood-risk.html#lognormal-distribution-with-3-parameters",
    "href": "qmd/14-flood-risk.html#lognormal-distribution-with-3-parameters",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "4 Lognormal Distribution with 3 Parameters",
    "text": "4 Lognormal Distribution with 3 Parameters\nLet’s repeat the procedure for a Lognormal distribution with three parameters. In this case the package FAdist is required. Results are presented in ?@fig-mle-ln3.\n\n## Fit a lognormal distribution with three parameters\nfitLn3 &lt;- mle(x=flow, dist=\"lnorm3\", start=c(0.5, 0.5, 0.5))\nfitLn3\n\n-----------------------------------------\n       Maximum Likelihood Estimates\n-----------------------------------------\nData object:  flow \nDistribution:  lnorm3 \n\n--------- Parameter estimates -----------\n\n         shape.hat scale.hat thres.hat\nEstimate    1.4640    6.3065    -1.369\nStd.err     0.1552    0.1874     5.165\n\n---------- Goodness-of-Fit --------------\n\n log.like       aic        ad       rho \n-518.5289 1043.0578    0.8941    0.9891 \n-----------------------------------------\n\n## diagnostic plots\nhist(flow, probability=TRUE)\nrug(flow)\nlines(density(flow))\nfunLn3 &lt;- function(flow) distr(x=flow, model=fitLn3, type=\"d\")\ncurve(funLn3, add=TRUE, col=\"red\")\n\nplot(x=fitLn3, ci=TRUE, alpha=0.05)\n\n## theroretical and empirical quantiles\nqlnorm3(p=0.95, shape=fitLn3$par.hat[1], scale=fitLn3$par.hat[2], thres=fitLn3$par.hat[3])\n\n[1] 6089.576\n\ndistr(x=0.95, dist=\"lnorm3\", param=c(fitLn3$par.hat[1], fitLn3$par.hat[2], fitLn3$par.hat[3]), type=\"q\")\n\n[1] 6089.576\n\nquantile(p=0.95, flow)\n\n 95% \n4589 \n\n## Fit Weibull distribution to the data\nhist(flow, probability=TRUE)\nfitW &lt;- mle(x=flow, dist=\"weibull\", start=c(0.1, 0.1))\nfitW\n\n-----------------------------------------\n       Maximum Likelihood Estimates\n-----------------------------------------\nData object:  flow \nDistribution:  weibull \n\n--------- Parameter estimates -----------\n\n         shape.hat scale.hat\nEstimate   0.82050      1070\nStd.err    0.07811       172\n\n---------- Goodness-of-Fit --------------\n\n log.like       aic        ad       rho \n-515.2496 1034.4993    0.3602    0.9681 \n-----------------------------------------\n\n## diagnostics\nfunW &lt;- function(flow) distr(x=flow, model=fitW, type=\"d\")\ncurve(funW, add=TRUE, col=\"blue\")\n\nplot(x=fitW, ci=TRUE, alpha=0.05)\n\n## quantiles\nqweibull(p=0.99, shape=fitW$par.hat[1], scale=fitW$par.hat[2])\n\n[1] 6884.165\n\ndistr(x=0.99, dist=\"weibull\", param=c(fitW$par.hat[1], fitW$par.hat[2]), type=\"q\")\n\n[1] 6884.165\n\nquantile(p=0.99, flow)\n\n 99% \n6974 \n\n## Which distribution is the best according to the AIC?\nfitLn2$aic\n\n[1] 1041.723\n\nfitLn3$aic\n\n[1] 1043.058\n\nfitW$aic\n\n[1] 1034.499\n\n\n\n\n\n\n\n\nFigure 4: Plot of the mle object corresponding to the fitting James River data using a LN3 distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Plot of the mle object corresponding to the fitting James River data using a LN3 distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Plot of the mle object corresponding to the fitting James River data using a LN3 distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Plot of the mle object corresponding to the fitting James River data using a LN3 distribution",
    "crumbs": [
      "Labs",
      "14-Extreme Value Estimation with Package FAmle"
    ]
  },
  {
    "objectID": "qmd/14-flood-risk.html#exercise-extreme-values-of-the-elbe-river",
    "href": "qmd/14-flood-risk.html#exercise-extreme-values-of-the-elbe-river",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "5 Exercise: Extreme values of the Elbe river",
    "text": "5 Exercise: Extreme values of the Elbe river\nNow load the Elbe River data from the beginning of the course and note that we need annual maximum values.\n\nelbe &lt;- read_csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/elbe.csv\", , col_types = c(\"D\", \"n\"))\n\n\n## annual maximum discharge\nelbe_annual &lt;-\n  mutate(elbe, year = year(date)) |&gt;\n  group_by(year) |&gt;\n  summarize(discharge = max(discharge))\n\nplot(discharge ~ year, data = elbe_annual)\n\n## check for trend and autocorrelation between years\nMannKendall(elbe_annual$discharge)\nacf(elbe_annual$discharge)\n\n\nfitLn3 &lt;- mle(x=elbe_annual$discharge, dist=\"lnorm3\", start=c(1, 5, 100))\nfitLn3\n\nflow &lt;- elbe_annual$discharge\n\nhist(flow, probability=TRUE, breaks = 10)\n\nrug(flow)\nlines(density(flow))\n\nxnew &lt;- seq(min(flow), max(flow), length = 100)\nfunLn3 &lt;- function(flow) distr(x=flow, model=fitLn3, type=\"d\")\nlines(xnew, funLn3(xnew), col=\"red\")\n\nImportant: The method described so far assumes stationarity of conditions, i.e. absence of meteorological and hydrological trends. Discuss, how climate warming already influences validity of the described method, and which methods need to be applied instead.",
    "crumbs": [
      "Labs",
      "14-Extreme Value Estimation with Package FAmle"
    ]
  },
  {
    "objectID": "qmd/03-distributions-leaves.html",
    "href": "qmd/03-distributions-leaves.html",
    "title": "03-Distribution and confidence intervals of maple leaf samples",
    "section": "",
    "text": "The example aims to demonstrate estimation and interpretation of confidence intervals. At the end, the two samples are compared with respect to variance and mean values.\nThe experimental hypotheses was, that the sampling strategy has an influence on the parameters of the distribution, i.e. that a sampling bias may occur. Here we leave it open, if the “subjective sampling” strategy prefers bigger or smaller leaves or if it has an influence on variance. The result is to be visualized with bar charts or box plots. We use only the leave width as an example, analysis of the other statistical parameters is left as an optional exercise.\nWe can now derive the following statistical hypotheses about the variance:\n\n\\(H_0\\): The variance of both samples is the same.\n\\(H_a\\): The samples have different variance.\n\nand about the mean:\n\n\\(H_0\\): The mean of both samples is the same.\n\\(H_a\\): The mean values of the samples are different.",
    "crumbs": [
      "Labs",
      "03-Distribution and confidence intervals of maple leaf samples"
    ]
  },
  {
    "objectID": "qmd/03-distributions-leaves.html#prepare-and-inspect-data",
    "href": "qmd/03-distributions-leaves.html#prepare-and-inspect-data",
    "title": "03-Distribution and confidence intervals of maple leaf samples",
    "section": "3.1 Prepare and inspect data",
    "text": "3.1 Prepare and inspect data\nThe data set is available from your local learning management system (LMS)) (e.g. OPAL at TU Dresden) or publicly from https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/leaves.csv.\n\nDownload the data set leaves.csv and use one of RStudio’s “Import Dataset” wizards.\nAlternative: use read.csv().\n\n\n#  ... do it\n\n\nplot everything, just for testing:\n\n\nplot(leaves)\n\n\nsplit table for HSE and MHYB:\n\n\nhyb &lt;- subset(leaves, group == \"HYB\")\nhse &lt;- subset(leaves, group == \"HSE\")\n\n\ncompare leaf width of both groups:\n\n\nboxplot(hse$width, hyb$width, names=c(\"HSE\", \"HYB\"))",
    "crumbs": [
      "Labs",
      "03-Distribution and confidence intervals of maple leaf samples"
    ]
  },
  {
    "objectID": "qmd/03-distributions-leaves.html#check-distribution",
    "href": "qmd/03-distributions-leaves.html#check-distribution",
    "title": "03-Distribution and confidence intervals of maple leaf samples",
    "section": "3.2 Check distribution",
    "text": "3.2 Check distribution\n\n# use `hist`, `qqnorm`, `qqline`\n# ...",
    "crumbs": [
      "Labs",
      "03-Distribution and confidence intervals of maple leaf samples"
    ]
  },
  {
    "objectID": "qmd/03-distributions-leaves.html#sample-statistics",
    "href": "qmd/03-distributions-leaves.html#sample-statistics",
    "title": "03-Distribution and confidence intervals of maple leaf samples",
    "section": "3.3 Sample statistics",
    "text": "3.3 Sample statistics\nIf we assume normal distribution of the data, we can estimate an approximate prediction interval from the sample parameters, i.e. in which size range are 95% of the leaves SAMPLE of one group. We first calculate mean, sd, N and se for “hse” data set:\n\nhse.mean &lt;- mean(hse$width)\nhse.sd   &lt;- sd(hse$width)\nhse.N    &lt;- length(hse$width)\nhse.se   &lt;- hse.sd/sqrt(hse.N)\n\nThen we estimate the two-sided 95% prediction interval for the sample, assuming normal distribution:\n\nhse.95 &lt;- hse.mean + c(-1.96, 1.96) * hse.sd\nhse.95\n\nInstead of using 1.96, we could also use the quantile function of the normal distribution instead, e.g. qnorm(0.975)for the upper interval or qnorm(c(0.025, 0.975)) for the lower and upper.\nIf the data set is large enough, we can compare the prediction interval from above with the empirical quantiles, i.e. take it directly from the data. Here we do not assume a normal or any other distribution.\n\nquantile(hse$width, p = c(0.025, 0.975))\n\nNow we plot the data and indicate the 95% interval:\n\nplot(hse$width)\nabline(h = hse.95, col=\"red\")\n\n… and the same as histogram:\n\nhist(hse$width)\nabline(v = hse.95, col=\"red\")\nrug(hse$width, col=\"blue\")",
    "crumbs": [
      "Labs",
      "03-Distribution and confidence intervals of maple leaf samples"
    ]
  },
  {
    "objectID": "qmd/03-distributions-leaves.html#confidence-interval-of-the-mean",
    "href": "qmd/03-distributions-leaves.html#confidence-interval-of-the-mean",
    "title": "03-Distribution and confidence intervals of maple leaf samples",
    "section": "3.4 Confidence interval of the mean",
    "text": "3.4 Confidence interval of the mean\nThe confidence interval of the mean tells us how precise a mean value was estimated from data. If the sample size is “large enough”, the distribution of the raw data does not necessarily need to be normal, because then mean values tend to approximate a normal distribution due to the central limit theorem.\n\n3.4.1 Confidence interval of the mean for the “hse” data\n\nCalculate the confidence interval of the mean value of the “hse” data set,\nuse +/- 1.96 or (better) the quantile of the t-distribution:\n\n\nhse.ci &lt;- hse.mean + qt(p = c(0.025, 0.975), df = hse.N-1) * hse.se\n\nNow indicate the confidence interval of the mean in the histogram.\n\nabline(v = hse.ci, col=\"red\")\n\n\n\n3.4.2 Confidence interval for the mean of the “hyb” data\n\n#  Do the same for the \"hyb\" data, calculate mean, sd, N, se and ci.\n# ...\n\n\n\n3.4.3 Visualization\nInstead of a boxplot, we can also use a bar chart with confidence interval. This can be done with the add-on package gplots (not to be confused with ggplot) or with some creativity\nSolution A) with package gplots\n\nlibrary(\"gplots\")\nbarplot2(height = c(hyb.mean, hse.mean),\n         ci.l   = c(hyb.ci[1], hse.ci[1]),\n         ci.u   = c(hyb.ci[2], hse.ci[2]),\n         plot.ci = TRUE,\n         names.arg=c(\"Hyb\", \"HSE\")\n)\n\nSolution B) without add-on packages (optional)\nHere we use a standard bar chart, and line segments for the error bars. One small problem arises, because barplot creates an own x-scaling. The good news is, that barplot returns its x-scale. We can store it in a variable, e.g. x that can then be used in subsequent code.\n\nx &lt;- barplot(c(hyb.mean, hse.mean),\n  names.arg=c(\"HYB\", \"HSE\"), ylim=c(0, 150))\nsegments(x0=x[1], y0=hyb.ci[1], y1=hyb.ci[2], lwd=2)\nsegments(x0=x[2], y0=hse.ci[1], y1=hse.ci[2], lwd=2)",
    "crumbs": [
      "Labs",
      "03-Distribution and confidence intervals of maple leaf samples"
    ]
  },
  {
    "objectID": "qmd/03-distributions-leaves.html#compare-samples-with-t--and-f-test",
    "href": "qmd/03-distributions-leaves.html#compare-samples-with-t--and-f-test",
    "title": "03-Distribution and confidence intervals of maple leaf samples",
    "section": "3.5 Compare samples with t- and F-Test",
    "text": "3.5 Compare samples with t- and F-Test\nHypotheses:\nNull: Both samples have the same mean width and variance.\nAlternative: The mean width (and possibly also the variance) differ because of more subjective sampling of HSE students. They may have prefered bigger or the nice small leaves.\n\nt.test(width ~ group, data = leaves)\n\nPerform also the classical t-test (var.equal=TRUE) and the F-test (var.test). Calculate absolute and relative effect size (mean differences) and interpret the results of all 3 tests.\n\n# var.test(...)\n# t.test(...)\n# ...",
    "crumbs": [
      "Labs",
      "03-Distribution and confidence intervals of maple leaf samples"
    ]
  },
  {
    "objectID": "qmd/03-distributions-leaves.html#calculation-of-summary-statistics-with-dplyr",
    "href": "qmd/03-distributions-leaves.html#calculation-of-summary-statistics-with-dplyr",
    "title": "03-Distribution and confidence intervals of maple leaf samples",
    "section": "4.1 Calculation of summary statistics with dplyr",
    "text": "4.1 Calculation of summary statistics with dplyr\n\nlibrary(\"dplyr\")\nleaves &lt;- read.csv(\"leaves.csv\")\n\nstats &lt;-\n  leaves %&gt;%\n    group_by(group) %&gt;%\n    summarize(mean = mean(width), sd=sd(width), N=length(width), se=sd/sqrt(N),\n              lwr = mean + qt(p = 0.025, df = N-1) * se,\n              upr = mean + qt(p = 0.975, df = N-1) * se\n             )\n\nstats",
    "crumbs": [
      "Labs",
      "03-Distribution and confidence intervals of maple leaf samples"
    ]
  },
  {
    "objectID": "qmd/03-distributions-leaves.html#barchart-and-errorbars-with-ggplot2",
    "href": "qmd/03-distributions-leaves.html#barchart-and-errorbars-with-ggplot2",
    "title": "03-Distribution and confidence intervals of maple leaf samples",
    "section": "4.2 Barchart and errorbars with ggplot2",
    "text": "4.2 Barchart and errorbars with ggplot2\n\nlibrary(\"ggplot2\")\nstats %&gt;%\n  ggplot(aes(x=group, y=mean, min=lwr, max=upr))  +\n    geom_col() + geom_errorbar(width=0.2)",
    "crumbs": [
      "Labs",
      "03-Distribution and confidence intervals of maple leaf samples"
    ]
  }
]